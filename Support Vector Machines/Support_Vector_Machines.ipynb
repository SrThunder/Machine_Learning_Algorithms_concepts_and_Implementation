{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "A Support Vector Machine (SVM) is a powerful supervised machine learning model used for classification. An SVM makes classifications by defining a decision boundary and then seeing what side of the boundary an unclassified point falls on. In the next few exercises, we’ll learn how these decision boundaries get defined, but for now, know that they’re defined by using a training set of classified points. That’s why SVMs are supervised machine learning models.\n",
    "\n",
    "Decision boundaries are easiest to wrap your head around when the data has two features. In this case, the decision boundary is a line. Take a look at the example below.\n",
    "\n",
    "<img src=\"images/two_dimensions.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Two clusters of points separated by a line\n",
    "Note that if the labels on the figures in this lesson are too small to read, you can resize this pane to increase the size of the images.\n",
    "\n",
    "This SVM is using data about fictional games of Quidditch from the Harry Potter universe! The classifier is trying to predict whether a team will make the playoffs or not. Every point in the training set represents a “historical” Quidditch team. Each point has two features — the average number of goals the team scores and the average number of minutes it takes the team to catch the Golden Snitch.\n",
    "\n",
    "After finding a decision boundary using the training set, you could give the SVM an unlabeled data point, and it will predict whether or not that team will make the playoffs.\n",
    "\n",
    "Decision boundaries exist even when your data has more than two features. If there are three features, the decision boundary is now a plane rather than a line.\n",
    "\n",
    "<img src=\"images/three_dimensions.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Two clusters of points in three dimensions separated by a plane.\n",
    "As the number of dimensions grows past 3, it becomes very difficult to visualize these points in space. Nonetheless, SVMs can still find a decision boundary. However, rather than being a separating line, or a separating plane, the decision boundary is called a separating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "Run the code to see two graphs appear. Right now they should be identical. We’re going to fix the bottom graph so it has a good decision boundary. Why is this decision boundary bad?\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "Let’s shift the line on the bottom graph to make it separate the two clusters. The slope of the line looks pretty good, so let’s keep that at -2.\n",
    "\n",
    "We want to move the boundary up, so change intercept_two so the line separates the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-757ec913f3e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Top graph intercept and slope\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graph'"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from graph import ax, x_1, y_1, x_2, y_2\n",
    "\n",
    "#Top graph intercept and slope\n",
    "intercept_one = 8\n",
    "slope_one = -2\n",
    "\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = intercept_one + slope_one * x_vals\n",
    "plt.plot(x_vals, y_vals, '-')\n",
    "\n",
    "#Bottom Graph\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.title('Good Decision Boundary')\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "plt.scatter(x_1, y_1, color = \"b\")\n",
    "plt.scatter(x_2, y_2, color = \"r\")\n",
    "\n",
    "#Change the intercept to separate the clusters\n",
    "intercept_two = 15\n",
    "slope_two = -2\n",
    "\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = intercept_two + slope_two * x_vals\n",
    "plt.plot(x_vals, y_vals, '-')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Decision Boundaries\n",
    "One problem that SVMs need to solve is figuring out what decision boundary to use. After all, there could be an infinite number of decision boundaries that correctly separate the two classes. Take a look at the image below:\n",
    "\n",
    "<img src=\"images/decision_boundaries.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "6 different valid decision boundaries\n",
    "There are so many valid decision boundaries, but which one is best? In general, we want our decision boundary to be as far away from training points as possible.\n",
    "\n",
    "Maximizing the distance between the decision boundary and points in each class will decrease the chance of false classification. Take graph C for example.\n",
    "\n",
    "<img src=\"images/graph_c.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "An SVM with a decision boundary very close to the blue points.\n",
    "The decision boundary is close to the blue class, so it is possible that a new point close to the blue cluster would fall on the red side of the line.\n",
    "\n",
    "Out of all the graphs shown here, graph F has the best decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "Run the code. Both graphs have suboptimal decision boundaries. Why? We’re going to fix the bottom graph.\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "We’re going to have to make the decision boundary much flatter, which means we first need to lower its y-intercept. Change intercept_two to be 8.\n",
    "\n",
    "3.\n",
    "Next, we want the slope to be pretty flat. Change the value of slope_two. The resulting line should split the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4043fd71a1d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Top graph intercept and slope\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graph'"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from graph import ax, x_1, y_1, x_2, y_2\n",
    "\n",
    "#Top graph intercept and slope\n",
    "intercept_one = 98\n",
    "slope_one = -20\n",
    "\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = intercept_one + slope_one * x_vals\n",
    "plt.plot(x_vals, y_vals, '-')\n",
    "\n",
    "#Bottom graph\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.title('Good Decision Boundary')\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "plt.scatter(x_1, y_1, color = \"b\")\n",
    "plt.scatter(x_2, y_2, color = \"r\")\n",
    "\n",
    "#Bottom graph intercept and slope\n",
    "intercept_two = 8\n",
    "slope_two = -0.5\n",
    "\n",
    "x_vals = np.array(ax.get_xlim())\n",
    "y_vals = intercept_two + slope_two * x_vals\n",
    "plt.plot(x_vals, y_vals, '-')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vectors and Margins\n",
    "\n",
    "We now know that we want our decision boundary to be as far away from our training points as possible. Let’s introduce some new terms that can help explain this idea.\n",
    "\n",
    "The support vectors are the points in the training set closest to the decision boundary. In fact, these vectors are what define the decision boundary. But why are they called vectors? Instead of thinking about the training data as points, we can think of them as vectors coming from the origin.\n",
    "\n",
    "<img src=\"images/vectors.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "Points represented as vectors.\n",
    "These vectors are crucial in defining the decision boundary — that’s where the “support” comes from. If you are using n features, there are at least n+1 support vectors.\n",
    "\n",
    "The distance between a support vector and the decision boundary is called the margin. We want to make the margin as large as possible. The support vectors are highlighted in the image below:\n",
    "\n",
    "<img src=\"images/margin.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "decision boundary with margin highlighted\n",
    "Because the support vectors are so critical in defining the decision boundary, many of the other training points can be ignored. This is one of the advantages of SVMs. Many supervised machine learning algorithms use every training point in order to make a prediction, even though many of those training points aren’t relevant. SVMs are fast because they only use the support vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "1.\n",
    "What are the support vectors for the SVM pictured below? There should be 2 blue support vectors and 1 red support vector.\n",
    "\n",
    "<img src=\"images/which_points.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "The blue points are at (2, 1), (2, 1), and (2.5, 2). The red points are at (1, 6), (1.5, 8), and (2, 7). The decision boundary is the line y = 4. \n",
    "Finish defining red_support_vector, blue_support_vector_one, and blue_support_vector_two. Set them equal to the correct points. The point should be represented as a list like [1, 0.5].\n",
    "\n",
    "2.\n",
    "What is the size of the margin? Find the total distance between a support vector and the line by looking at the graph. Create a variable named margin_size and set it equal to the correct number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_support_vector = [1.0, 6]\n",
    "blue_support_vector_one = [0.5, 2]\n",
    "blue_support_vector_two = [2.5, 2]\n",
    "\n",
    "\n",
    "margin_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn\n",
    "\n",
    "Now that we know the concepts behind SVMs we need to write the code that will find the decision boundary that maximizes the margin. All of the code that we’ve written so far has been guessing and checking — we don’t actually know if we’ve found the best line. Unfortunately, calculating the parameters of the best decision boundary is a fairly complex optimization problem. Luckily, Python’s scikit-learn library has implemented an SVM that will do this for us.\n",
    "\n",
    "Note that while it is not important to understand how the optimal parameters are found, you should have a strong conceptual understanding of what the model is optimizing.\n",
    "\n",
    "To use scikit-learn’s SVM we first need to create an SVC object. It is called an SVC because scikit-learn is calling the model a Support Vector Classifier rather than a Support Vector Machine.\n",
    "\n",
    "### classifier = SVC(kernel = 'linear')\n",
    "\n",
    "We’ll soon go into what the kernel parameter is doing, but for now, let’s use a 'linear' kernel.\n",
    "\n",
    "Next, the model needs to be trained on a list of data points and a list of labels associated with those data points. The labels are analogous to the color of the point — you can think of a 1 as a red point and a 0 as a blue point. The training is done using the .fit() method:\n",
    "\n",
    "### training_points = [[1, 2], [1, 5], [2, 2], [7, 5], [9, 4], [8, 2]]\n",
    "\n",
    "### labels = [1, 1, 1, 0, 0, 0]\n",
    "\n",
    "### classifier.fit(training_points, labels) \n",
    "\n",
    "The graph of this dataset would look like this:\n",
    "\n",
    "<img src=\"images/example_dataset.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "An SVM with a decision boundary very close to the blue points.\n",
    "\n",
    "Calling .fit() creates the line between the points.\n",
    "\n",
    "Finally, the classifier predicts the label of new points using the .predict() method. The .predict() method takes a list of points you want to classify. Even if you only want to classify one point, make sure it is in a list:\n",
    "\n",
    "### print(classifier.predict([[3, 2]]))\n",
    "\n",
    "In the image below, you can see the unclassified point [3, 2] as a black dot. It falls on the red side of the line, so the SVM would predict it is red.\n",
    "\n",
    "<img src=\"images/predict.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "An SVM with a decision boundary very close to the blue points.\n",
    "\n",
    "In addition to using the SVM to make predictions, you can inspect some of its attributes. For example, if you can print classifier.support_vectors_ to see which points from the training set are the support vectors.\n",
    "\n",
    "In this case, the support vectors look like this:\n",
    "\n",
    "[[7, 5],\n",
    " [8, 2],\n",
    " [2, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "1.\n",
    "Let’s start by making a SVC object with kernel = 'linear'. Name the object classifier.\n",
    "\n",
    "\n",
    "2.\n",
    "We’ve imported the training set and labels for you. Call classifier‘s .fit() method using points and labels as parameters.\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "We can now classify new points. Try classifying both [3, 4] and [6, 7]. Remember, the .predict() function expects a list of points to predict.\n",
    "\n",
    "Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-58cbbf465286>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graph'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from graph import points, labels\n",
    "\n",
    "classifier = SVC(kernel='linear')\n",
    "\n",
    "classifier.fit(points,labels)\n",
    "\n",
    "results=classifier.predict([[3, 4],[6, 7]])\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "\n",
    "SVMs try to maximize the size of the margin while still correctly separating the points of each class. As a result, outliers can be a problem. Consider the image below.\n",
    "\n",
    "<img src=\"images/outliers.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "One graph with a hard margin and one graph with a soft margin\n",
    "\n",
    "The size of the margin decreases when a single outlier is present, and as a result, the decision boundary changes as well. However, if we allowed the decision boundary to have some error, we could still use the original line.\n",
    "\n",
    "SVMs have a parameter C that determines how much error the SVM will allow for. If C is large, then the SVM has a hard margin — it won’t allow for many misclassifications, and as a result, the margin could be fairly small. If C is too large, the model runs the risk of overfitting. It relies too heavily on the training data, including the outliers.\n",
    "\n",
    "On the other hand, if C is small, the SVM has a soft margin. Some points might fall on the wrong side of the line, but the margin will be large. This is resistant to outliers, but if C gets too small, you run the risk of underfitting. The SVM will allow for so much error that the training data won’t be represented.\n",
    "\n",
    "When using scikit-learn’s SVM, you can set the value of C when you create the object:\n",
    "\n",
    "### classifier = SVC(C = 0.01)\n",
    "\n",
    "The optimal value of C will depend on your data. Don’t always maximize margin size at the expense of error. Don’t always minimize error at the expense of margin size. The best strategy is to validate your model by testing many different values for C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "1.\n",
    "Run the code to see the SVM’s current boundary line. Note that we’ve imported some helper functions we wrote named draw_points and draw_margins to help visualize the SVM.\n",
    "\n",
    "2.\n",
    "Let’s add an outlier! Before calling .fit(), append [3, 3] to points and append 0 to labels. This will add a blue point at [3, 3]\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Right now, our classifier has hard margins because C = 1. Change the value of C to 0.01 to see what the SVM looks like with soft margins.\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "append at least two more points to points. If you want the points to appear on the graph, make sure their x and y values are between 0 and 12.\n",
    "\n",
    "Make sure to also append a label to labels for every point you add. A 0 will make the point blue and a 1 will make the point red.\n",
    "\n",
    "Make sure to add the points before training the SVM.\n",
    "\n",
    "\n",
    "\n",
    "5.\n",
    "Play around with the C variable to see how the decision boundary changes with your new points added. Change C to be a value between 0.01 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-95f88db7877f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraw_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraw_margin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graph'"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from graph import points, labels, draw_points, draw_margin\n",
    "\n",
    "points.append([3, 3])\n",
    "labels.append(0)\n",
    "\n",
    "points.append([10, 8])\n",
    "labels.append(1)\n",
    "\n",
    "points.append([11, 7])\n",
    "labels.append(1)\n",
    "\n",
    "classifier = SVC(kernel='linear', C = 0.5)\n",
    "classifier.fit(points, labels)\n",
    "\n",
    "draw_points(points, labels)\n",
    "draw_margin(classifier)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
