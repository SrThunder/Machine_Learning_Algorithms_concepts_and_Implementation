{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION\n",
    "# Loss\n",
    "When we think about how we can assign a slope and intercept to fit a set of points, we have to define what the best fit is.\n",
    "\n",
    "For each data point, we calculate loss, a number that measures how bad the model’s (in this case, the line’s) prediction was. You may have seen this being referred to as error.\n",
    "\n",
    "We can think about loss as the squared distance from the point to the line. We do the squared distance (instead of just the distance) so that points above and below the line both contribute to total loss in the same way:\n",
    "\n",
    "\n",
    "In this example:\n",
    "\n",
    "* For point A, the squared distance is 9 (3²)\n",
    "* For point B, the squared distance is 1 (1²)\n",
    "* So the total loss, with this model, is 10. If we found a line that had less loss than 10, that line would be a better       model for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 13.5 13.5\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [5, 1, 3]\n",
    "\n",
    "#y = x\n",
    "m1 = 1\n",
    "b1 = 0\n",
    "\n",
    "#y = 0.5x + 1\n",
    "m2 = 0.5\n",
    "b2 = 1\n",
    "\n",
    "m3 = 5\n",
    "b3 = 2\n",
    "\n",
    "y_predicted1 = [m1*x_val + b1 for x_val in x]\n",
    "y_predicted2 = [m2*x_val + b2 for x_val in x]\n",
    "y_predicted3 = [m3*x_val + b3 for x_val in x]\n",
    "total_loss1 = 0\n",
    "total_loss2 = 0\n",
    "total_loss3 = 0\n",
    "\n",
    "for i in range(len(y)):\n",
    "  total_loss1 += (y[i] - y_predicted1[i])**2\n",
    "  total_loss2 += (y[i] - y_predicted2[i])**2\n",
    "  total_loss3 += (y[i] - y_predicted3[i])**2\n",
    "  \n",
    "print(total_loss1, total_loss2,total_loss2)\n",
    "better_fit = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Intercept\n",
    "***\n",
    "To find the gradient of loss as intercept changes, the formula comes out to be:\n",
    "\n",
    "$\n",
    "\\frac{2}{N}\\sum_{i=1}^{N}-(y_i-(mx_i+b)) \n",
    "$\n",
    "\n",
    "* N is the number of points we have in our dataset\n",
    "* m is the current gradient guess\n",
    "* b is the current intercept guess\n",
    "Basically:\n",
    "\n",
    "we find the sum of y_value - (m*x_value + b) for all the y_values and x_values we have\n",
    "and then we multiply the sum by a factor of -2/N. N is the number of points we have.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_at_b(x,y,m,b):\n",
    "  # diff = ([lambda x1, y1: y1-((m*x1)+b) for x1, y1 in range(x,y) ])\n",
    "  diff =0\n",
    "  for i in range(len(x)):\n",
    "    x_val = x[i]\n",
    "    y_val = y[i]\n",
    "    diff+= y_val - ((m*x_val)+b)\n",
    "  N = len(x)\n",
    "  b_gradient = -2/N * diff\n",
    "  \n",
    "  print(b_gradient)\n",
    "  \n",
    "  return b_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Slope\n",
    "***\n",
    "We have a function to find the gradient of b at every point. To find the m gradient, or the way the loss changes as the slope of our line changes, we can use this formula:\n",
    "\n",
    "$ \\frac{2}{N}\\sum_{i=1}^{N}-x_i(y_i-(mx_i+b)) $\n",
    "\n",
    "Once more:\n",
    "\n",
    "* N is the number of points you have in your dataset\n",
    "* m is the current gradient guess\n",
    "* b is the current intercept guess\n",
    "* To find the m gradient:\n",
    "\n",
    "we find the sum of x_value * (y_value - (m*x_value + b)) for all the y_values and x_values we have\n",
    "and then we multiply the sum by a factor of -2/N. N is the number of points we have.\n",
    "Once we have a way to calculate both the m gradient and the b gradient, we’ll be able to follow both of those gradients downwards to the point of lowest loss for both the m value and the b value. Then, we’ll have the best m and the best b to fit our data!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_at_m(x, y, m, b):\n",
    "    diff = 0\n",
    "    N = len(x)\n",
    "    for i in range(N):\n",
    "      y_val = y[i]\n",
    "      x_val = x[i]\n",
    "      diff += x_val*(y_val - ((m * x_val) + b))\n",
    "    m_gradient = -2/N * diff\n",
    "    return m_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it Together\n",
    "Now that we know how to calculate the gradient, we want to take a “step” in that direction. However, it’s important to think about whether that step is too big or too small. We don’t want to overshoot the minimum error!\n",
    "\n",
    "We can scale the size of the step by multiplying the gradient by a learning rate.\n",
    "\n",
    "To find a new b value, we would say:\n",
    "\n",
    "# new_b = current_b - (learning_rate * b_gradient) \n",
    "where current_b is our guess for what the b value is, b_gradient is the gradient of the loss curve at our current guess, and learning_rate is proportional to the size of the step we want to take.\n",
    "\n",
    "In a few exercises, we’ll talk about the implications of a large or small learning rate, but for now, let’s use a fairly small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.355 17.78333333333333\n"
     ]
    }
   ],
   "source": [
    "def get_gradient_at_b(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "    x_val = x[i]\n",
    "    y_val = y[i]\n",
    "    diff += (y_val - ((m * x_val) + b))\n",
    "  b_gradient = -(2/N) * diff  \n",
    "  return b_gradient\n",
    "\n",
    "def get_gradient_at_m(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "      x_val = x[i]\n",
    "      y_val = y[i]\n",
    "      diff += x_val * (y_val - ((m * x_val) + b))\n",
    "  m_gradient = -(2/N) * diff  \n",
    "  return m_gradient\n",
    "\n",
    "#Your step_gradient function here\n",
    "def step_gradient(x, y, b_current, m_current):\n",
    "    b_gradient = get_gradient_at_b(x, y, b_current, m_current)\n",
    "    m_gradient = get_gradient_at_m(x, y, b_current, m_current)\n",
    "    b = b_current - (0.01 * b_gradient)\n",
    "    m = m_current - (0.01 * m_gradient)\n",
    "    return [b, m]\n",
    "\n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]\n",
    "\n",
    "# current intercept guess:\n",
    "b = 0\n",
    "# current slope guess:\n",
    "m = 0\n",
    "\n",
    "b, m = step_gradient(months, revenue, b, m)\n",
    "print(b, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence\n",
    "How do we know when we should stop changing the parameters m and b? How will we know when our program has learned enough?\n",
    "\n",
    "To answer this, we have to define convergence. Convergence is when the loss stops changing (or changes very slowly) when parameters are changed.\n",
    "\n",
    "Hopefully, the algorithm will converge at the best values for the parameters m and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate\n",
    "We want our program to be able to iteratively learn what the best m and b values are. So for each m and b pair that we guess, we want to move them in the direction of the gradients we’ve calculated. But how far do we move in that direction?\n",
    "\n",
    "We have to choose a learning rate, which will determine how far down the loss curve we go.\n",
    "\n",
    "A small learning rate will take a long time to converge — you might run out of time or cycles before getting an answer. A large learning rate might skip over the best value. It might never converge! Oh no!\n",
    "\n",
    "\n",
    "Finding the absolute best learning rate is not necessary for training a model. You just have to find a learning rate large enough that gradient descent converges with the efficiency you need, and not so large that convergence never happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION\n",
    "## Put it Together II \n",
    "At each step, we know how to calculate the gradient and move in that direction with a step size proportional to our learning rate. Now, we want to make these steps until we reach convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "We have all of the functions we have defined throughout the lesson.\n",
    "\n",
    "Now, let’s create a function called gradient_descent() that takes in x, y, learning_rate, and a num_iterations.\n",
    "\n",
    "For now, return [-1,-1].\n",
    "\n",
    "2.\n",
    "In the function gradient_descent(), create variables b and m and set them both to zero for our initial guess.\n",
    "\n",
    "Return b and m from the function.\n",
    "\n",
    "3.\n",
    "Update your step_gradient() function to take in the parameter learning_rate (as the last parameter) and replace the 0.01s in the calculations of b_gradient and m_gradient with learning_rate.\n",
    "\n",
    "\n",
    "4.\n",
    "Let’s go back and finish the gradient_descent() function.\n",
    "\n",
    "Create a loop that runs num_iterations times. At each step, it should:\n",
    "\n",
    "Call step_gradient() with b, m, x, y, and learning_rate\n",
    "Update the values of b and m with the values step_gradient() returns.\n",
    "5.\n",
    "Outside of the function, the line that calls gradient_descent on months and revenue, with a learning rate of 0.01 and 1000 iterations.\n",
    "\n",
    "It stores the results in variables called b and m.\n",
    "\n",
    "6.\n",
    "the lines that will plot the result to the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_gradient_at_b(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "    x_val = x[i]\n",
    "    y_val = y[i]\n",
    "    diff += (y_val - ((m * x_val) + b))\n",
    "  b_gradient = -(2/N) * diff  \n",
    "  return b_gradient\n",
    "\n",
    "def get_gradient_at_m(x, y, b, m):\n",
    "  N = len(x)\n",
    "  diff = 0\n",
    "  for i in range(N):\n",
    "      x_val = x[i]\n",
    "      y_val = y[i]\n",
    "      diff += x_val * (y_val - ((m * x_val) + b))\n",
    "  m_gradient = -(2/N) * diff  \n",
    "  return m_gradient\n",
    "\n",
    "#Your step_gradient function here\n",
    "def step_gradient(b_current, m_current, x, y, learning_rate):\n",
    "    b_gradient = get_gradient_at_b(x, y, b_current, m_current)\n",
    "    m_gradient = get_gradient_at_m(x, y, b_current, m_current)\n",
    "    b = b_current - (learning_rate * b_gradient)\n",
    "    m = m_current - (learning_rate * m_gradient)\n",
    "    return [b, m]\n",
    "  \n",
    "#Your gradient_descent function here:  \n",
    "def gradient_descent(x, y, learning_rate, num_iterations):\n",
    "  b = 0\n",
    "  m = 0\n",
    "  for i in range(num_iterations):\n",
    "    b, m = step_gradient(b, m, x, y, learning_rate)\n",
    "  return [b,m]  \n",
    "\n",
    "months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]\n",
    "\n",
    "#Uncomment the line below to run your gradient_descent function\n",
    "b, m = gradient_descent(months, revenue, 0.01, 1000)\n",
    "\n",
    "#Uncomment the lines below to see the line you've settled upon!\n",
    "y = [m*x + b for x in months]\n",
    "\n",
    "plt.plot(months, revenue, \"o\")\n",
    "plt.plot(months, y)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Your Functions on Real Data\n",
    "We have constructed a way to find the “best” b and m values using gradient descent! Let’s try this on the set of baseball players’ heights and weights that we saw at the beginning of the lesson.\n",
    "\n",
    "1.\n",
    "Run the code in script.py.\n",
    "\n",
    "This is a scatterplot of weight vs height.\n",
    "\n",
    "2.\n",
    "We have imported your gradient_descent() function. Call it with parameters:\n",
    "\n",
    "* X\n",
    "* y\n",
    "* num_iterations of 1000\n",
    "* learning_rate of 0.0001\n",
    "* Store the result in variables called b and m.\n",
    "\n",
    "\n",
    "3.\n",
    "Create a list called y_predictions. Set it to be every element of X multiplied by m and added to b.\n",
    "\n",
    "The easiest way to do this would be a list comprehension:\n",
    "\n",
    "new_y = [element*slope + intercept for element in y]\n",
    "\n",
    "\n",
    "4.\n",
    "Plot X vs y_predictions on the same plot as the scatterplot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from gradient_descent_funcs import gradient_descent\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"height_weight.csv\")\n",
    "\n",
    "X = df[\"height\"]\n",
    "y = df[\"weight\"]\n",
    "b, m = gradient_descent(X, y, num_iterations = 1000, learning_rate = 0.0001 )\n",
    "y_predictions = [m*x + b for x in X]\n",
    "plt.plot(X, y, 'o')\n",
    "#plot your line here:\n",
    "plt.plot(X, y_predictions)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION\n",
    "## Scikit-Learn\n",
    "### Congratulations! You’ve now built a linear regression algorithm from scratch.\n",
    "\n",
    "Luckily, we don’t have to do this every time we want to use linear regression. We can use Python’s scikit-learn library. Scikit-learn, or sklearn, is used specifically for Machine Learning. Inside the linear_model module, there is a LinearRegression() function we can use:\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "You can first create a LinearRegression model, and then fit it to your x and y data:\n",
    "\n",
    "### line_fitter = LinearRegression()\n",
    "### line_fitter.fit(X, y)\n",
    "The .fit() method gives the model two variables that are useful to us:\n",
    "\n",
    "the line_fitter.coef_, which contains the slope\n",
    "the line_fitter.intercept_, which contains the intercept\n",
    "We can also use the .predict() function to pass in x-values and receive the y-values that this line would predict:\n",
    "\n",
    "### y_predicted = line_fitter.predict(X)\n",
    "\n",
    "Note: the num_iterations and the learning_rate that you learned about in your own implementation have default values within scikit-learn, so you don’t need to worry about setting them specifically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "temperature = np.array(range(60, 100, 2))\n",
    "temperature = temperature.reshape(-1, 1)\n",
    "sales = [65, 58, 46, 45, 44, 42, 40, 40, 36, 38, 38, 28, 30, 22, 27, 25, 25, 20, 15, 5]\n",
    "line_fitter = LinearRegression()\n",
    "line_fitter.fit(temperature, sales)\n",
    "sales_predict  = line_fitter.predict(temperature)\n",
    "print(sales_predict)\n",
    "plt.plot(temperature, sales, 'o')\n",
    "plt.plot(temperature, sales_predict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
