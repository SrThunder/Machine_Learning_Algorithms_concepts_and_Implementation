{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAYES' THEOREM\n",
    "\n",
    "## Independent Events\n",
    "\n",
    "The ability to determine whether two events are independent is an important skill for statistics.\n",
    "\n",
    "If two events are independent, then the occurrence of one event does not affect the probability of the other event. Here are some examples of independent events:\n",
    "\n",
    "I wear a blue shirt; my coworker wears a blue shirt\n",
    "I take the subway to work; I eat sushi for lunch\n",
    "The NY Giants win their football game; the NY Rangers win their hockey game\n",
    "If two events are dependent, then when one event occurs, the probability of the other event occurring changes in a predictable way.\n",
    "\n",
    "Here are some examples of dependent events:\n",
    "\n",
    "It rains on Tuesday; I carry an umbrella on Tuesday\n",
    "I eat spaghetti; I have a red stain on my shirt\n",
    "I wear sunglasses; I go to the beach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "A certain family plans to have three children. Is the event that the couple’s third child is a girl independent of the event that the couple’s first two children are girls?\n",
    "\n",
    "Save your answer ('independent' or 'not independent') to the variable third_child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_child = 'independent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Probability\n",
    "\n",
    "Conditional probability is the probability that two events happen. It’s easiest to calculate conditional probability when the two events are independent.\n",
    "\n",
    "Note: For the rest of this lesson, we’ll be using the statistical convention that the probability of an event is written as P(event).\n",
    "\n",
    "If the probability of event A is P(A) and the probability of event B is P(B) and the two events are independent, then the probability of both events occurring is the product of the probabilities:\n",
    "\n",
    "P(A ∩ B) = P(A) \\times P(B)P(A∩B)=P(A)×P(B)\n",
    "The symbol ∩ just means “and”, so P(A ∩ B) means the probability that both A and B happen.\n",
    "\n",
    "For instance, suppose we are rolling a pair of dice, and want to know the probability of rolling two sixes.\n",
    "\n",
    "<img src=\"images/dice.png\" style=\"width: 400;\"/>\n",
    "\n",
    "\n",
    "\n",
    "Each die has six sides, so the probability of rolling a six is 1/6. Each die is independent (i.e., rolling one six does not increase or decrease our chance of rolling a second six), so:\n",
    "\n",
    "$ P(6 \\cap 6) = P(6) \\times P(6) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "This week, there is a 30% probability that it will rain on any given day. At a certain high school, gym class is held on three days out of the five day school week.\n",
    "\n",
    "On a school day, what is the probability that it is raining and the students have gym class?\n",
    "\n",
    "Save your answer to the variable p_rain_and_gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hint\n",
    "\n",
    "$P(rain)=0.30$\n",
    "\n",
    "$P(gym) = \\frac{3.0}{5.0} = 0.60P $\n",
    "\n",
    "Because these two events are independent:\n",
    "\n",
    "$ P(rain ∩ gym) = P(rain) \\times P(gym)P(rain∩gym)=P(rain)×P(gym) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_rain_and_gym = .30 * (3.0/5.0)\n",
    "\n",
    "print(p_rain_and_gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for a Rare Disease\n",
    "\n",
    "Suppose you are a doctor and you need to test if a patient has a certain rare disease. The test is very accurate: it’s correct 99% of the time. The disease is very rare: only 1 in 100,000 patients have it.\n",
    "\n",
    "You administer the test and it comes back positive, so your patient must have the disease, right?\n",
    "\n",
    "Not necessarily. If we just consider the test, there is only a 1% chance that it is wrong, but we actually have more information: we know how rare the disease is.\n",
    "\n",
    "Given that the test came back positive, there are two possibilities:\n",
    "\n",
    "The patient had the disease, and the test correctly diagnosed the disease.\n",
    "The patient didn’t have the disease and the test incorrectly diagnosed that they had the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "What is the probability that the patient had the disease and the test correctly diagnosed the disease?\n",
    "\n",
    "Save your answer to the variable p_disease_and_correct.\n",
    "\n",
    "\n",
    "Hint\n",
    "The disease is rare, so the probability that the patient had the disease is 1 out of 100,000:\n",
    "\n",
    "$ P(disease) = \\frac{1}{100000}$\n",
    " \n",
    "The test is only wrong 1% of the time, so it is correct 99% of the time:\n",
    "\n",
    "$P(test\\ is\\ correct) = 0.99$ \n",
    "\n",
    "So the answer should look like:\n",
    "\n",
    "$p_disease_and_correct = (1.0 / 100000) * 0.99$\n",
    "\n",
    "2.\n",
    "What is the probability that the patient does not have the disease and the test incorrectly diagnosed the disease?\n",
    "\n",
    "Save your answer to the variable p_no_disease_and_incorrect.\n",
    "\n",
    "\n",
    "Hint\n",
    "The disease is rare, so the probability that the patient does not have the disease the disease is 99,999 out of 100,000:\n",
    "\n",
    "$P(disease) = \\frac{99999}{100000}$\n",
    "\t \n",
    "The test is only wrong 1% of the time:\n",
    "\n",
    "$P(test\\ is\\ correct) = 0.01$\n",
    "\n",
    "So the answer should look like:\n",
    "\n",
    "p_no_disease_and_incorrect = $(99999.0 / 100000) * 0.01$\n",
    "\n",
    "3.\n",
    "Print both p_disease_and_correct and p_no_disease_and_incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.9e-06\n",
      "0.0099999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_disease_and_correct = .99 * (1.0/100000)\n",
    "\n",
    "print(p_disease_and_correct)\n",
    "\n",
    "p_no_disease_and_incorrect = (99999.0/100000) * .01\n",
    "\n",
    "print(p_no_disease_and_incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "1.\n",
    "Calculate P(positive result | rare disease), or the probability of a positive test result, given that a patient really has this rare disease.\n",
    "\n",
    "Save your answer (as a decimal) to p_positive_given_disease.\n",
    "\n",
    "\n",
    "Hint\n",
    "The test is 99% accurate; given the fact that the patient has the disease, we know that there is a 99% probability that the test will return a positive result.\n",
    "\n",
    "This is exactly $P(positive result | rare disease)$.\n",
    "\n",
    "So the answer should look like:\n",
    "\n",
    "p_positive_given_disease = 0.99\n",
    "2.\n",
    "What is P(rare disease), the probability that a randomly selected patient has the rare disease?\n",
    "\n",
    "Save your answer to p_disease.\n",
    "\n",
    "\n",
    "Hint\n",
    "The disease is very rare. Only 1 in 100,000 people have it.\n",
    "\n",
    "3.\n",
    "We now need to compute the denominator; we need to find P(positive result).\n",
    "\n",
    "As we discussed previously, there are two ways to get a positive result:\n",
    "\n",
    "The patient had the disease, and the test correctly diagnosed the disease.\n",
    "The patient didn’t have the disease and the test incorrectly diagnosed that they had the disease.\n",
    "Using these two probabilities, calculate the total probability that a randomly selected patient receives a positive test result, P(positive result).\n",
    "\n",
    "Save your answer to the variable p_positive.\n",
    "\n",
    "\n",
    "Hint\n",
    "The probability that the patient had the disease, and the test correctly diagnosed the disease is:\n",
    "\n",
    "$1.0 / 100000.0 * 0.99$\n",
    "\n",
    "The probability that the patient didn’t have the disease and the test incorrectly diagnosed that they had the disease is:\n",
    "\n",
    "$99999.0 / 100000.0 * 0.01$\n",
    "\n",
    "The probability of either event A or event B happening is given by:\n",
    "\n",
    "$P(A\\ or\\ B) = P(A) + P(B)$ \n",
    "\n",
    "4.\n",
    "Substitute all three of these values into Bayes’ Theorem and calculate $P(rare disease | positive result)$.\n",
    "\n",
    "Save your result as p_disease_given_positive.\n",
    "\n",
    "\n",
    "Hint\n",
    "The numerator should be (p_positive_given_disease) * (p_disease).\n",
    "\n",
    "The denominator should be p_positive.\n",
    "\n",
    "5.\n",
    "Print p_disease_given_positive.\n",
    "\n",
    "Is it likely that your patient has this disease?\n",
    "\n",
    "\n",
    "Hint\n",
    "print p_disease_given_positive\n",
    "The result should look something like:\n",
    "\n",
    "0.000989010989011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9899999999999999\n",
      "1e-05\n",
      "0.01001\n",
      "0.000989010989010989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# P(positive result | rare disease)\n",
    "p_positive_given_disease = (0.99 * (.00001))/ (1./100000.)\n",
    "print(p_positive_given_disease)\n",
    "\n",
    "# P(rare disease)\n",
    "p_disease = 1./100000.\n",
    "print(p_disease)\n",
    "\n",
    "\n",
    "# P(positive result)\n",
    "p_positive = (0.00001) + (0.01) \n",
    "print(p_positive)\n",
    "\n",
    "\n",
    "# P(rare disease | positive result)\n",
    "# result\n",
    "p_disease_given_positive = (p_positive_given_disease) * (p_disease) / (p_positive)\n",
    "\n",
    "print(p_disease_given_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Filters\n",
    "Let’s explore a different example. Email spam filters use Bayes’ Theorem to determine if certain words indicate that an email is spam.\n",
    "\n",
    "Let’s a take word that often appears in spam: “enhancement”.\n",
    "\n",
    "With just 3 facts, we can make some preliminary steps towards a good spam filter:\n",
    "\n",
    "1. “enhancement” appears in just 0.1% of non-spam emails\n",
    "\n",
    "2. “enhancement” appears in 5% of spam emails\n",
    "\n",
    "3. Spam emails make up about 20% of total emails\n",
    "\n",
    "Given that an email contains “enhancement”, what is the probability that the email is spam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "In this example, we are dealing with two probabilities:\n",
    "\n",
    "P(enhancement) - the probability that the word “enhancement” appears in an email.\n",
    "\n",
    "P(spam) - the probability that an email is spam.\n",
    "\n",
    "Using Bayes’ Theorem to answer our question means that we want to calculate P(A|B).\n",
    "\n",
    "But what are A and B referring to in this case?\n",
    "\n",
    "Save the string 'enhancement' or 'spam' to the variable a.\n",
    "\n",
    "Save the string 'enhancement' or 'spam' to the variable b.\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "What is P(spam)?\n",
    "\n",
    "Save your answer to p_spam.\n",
    "\n",
    "\n",
    "Stuck? Get a hint\n",
    "3.\n",
    "What is P(enhancement | spam)?\n",
    "\n",
    "Save your answer to p_enhancement_given_spam.\n",
    "\n",
    "\n",
    "Stuck? Get a hint\n",
    "4.\n",
    "We want to know the overall probability that any email (spam or non-spam) contains “enhancement”.\n",
    "\n",
    "Because we know the probability of “enhancement” occurring in both spam (0.05) and non-spam (0.001) emails, we can use a weighted average to calculate the probability of “enhancement” occurring in an email:\n",
    "\n",
    "$P(enhancement) = P(enhancement\\ |\\ spam) \\times P(spam) + P(enhacement\\ |\\ not\\ spam) \\times P(not\\ spam)$\n",
    "\n",
    "Save your answer to p_enhancement.\n",
    "\n",
    "\n",
    "Hint\n",
    "The probability that an email is not spam is just 1 - the probability that the email is spam:\n",
    "\n",
    "$P(not\\ spam) = 1 - P(spam) $\n",
    "\n",
    "$P(enhancement | non-spam)$ is given to you above.\n",
    "\n",
    "5.\n",
    "Now that we know:\n",
    "\n",
    "P(spam)\n",
    "P(enhancement | spam)\n",
    "P(enhancement)\n",
    "\n",
    "We can plug this into Bayes’ Theorem:\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$ \n",
    "\n",
    "\n",
    "Save your answer as p_spam_enhancement.\n",
    "\n",
    "\n",
    "\n",
    "6.\n",
    "Print p_spam_enhancement. This is the probability that an email is spam given that it contains the word “enhancement”.\n",
    "\n",
    "Should we block all emails that contain “enhancement”?\n",
    "\n",
    "How much non-spam email would we block?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = 'spam'\n",
    "b = 'enhancement'\n",
    "\n",
    "p_spam = 0.2\n",
    "\n",
    "p_enhancement_given_spam = 0.05\n",
    "\n",
    "p_enhancement = 0.05 * 0.2 + 0.001 * (1 - 0.2)\n",
    "\n",
    "p_spam_enhancement = p_enhancement_given_spam * p_spam / p_enhancement\n",
    "\n",
    "print(p_spam_enhancement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "In this course, we learned several new definitions:\n",
    "\n",
    "* Two events are independent if the occurrence of one event does not affect the probability of the second event\n",
    "\n",
    "* If two events are independent then:\n",
    "\n",
    "   $P(A ∩ B) = P(A) \\times P(B)$ \n",
    "   \n",
    "\n",
    "* A prior is an additional piece of information that tells us how likely an event is\n",
    "\n",
    "* A frequentist approach to statistics does not incorporate a prior\n",
    "\n",
    "* A Bayesian approach to statistics incorporates prior knowledge\n",
    "\n",
    "* Bayes’ Theorem is the following:\n",
    "\n",
    "   $P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n",
    "   \n",
    "   \n",
    " <img src=\"images/bayes.svg\" style=\"width: 400;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes Classifier\n",
    "A Naive Bayes classifier is a supervised machine learning algorithm that leverages Bayes’ Theorem to make predictions and classifications. Recall Bayes’ Theorem:\n",
    "\n",
    "$P(A\\ | \\ B) = \\frac{P(B\\ |\\ A) \\cdot P(A)}{P(B)}$\t\n",
    "\n",
    "This equation is finding the probability of A given B. This can be turned into a classifier if we replace B with a data point and A with a class. For example, let’s say we’re trying to classify an email as either spam or not spam. We could calculate P(spam | email) and P(not spam | email). Whichever probability is higher will be the classifier’s prediction. Naive Bayes classifiers are often used for text classification.\n",
    "\n",
    "So why is this a supervised machine learning algorithm? In order to compute the probabilities used in Bayes’ theorem, we need previous data points. For example, in the spam example, we’ll need to compute P(spam). This can be found by looking at a tagged dataset of emails and finding the ratio of spam to non-spam emails.\n",
    "\n",
    " <img src=\"images/spam-classifier.gif\" style=\"width: 400;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate the Data\n",
    "\n",
    "In this lesson, we are going to create a Naive Bayes classifier that can predict whether a review for a product is positive or negative. This type of classifier could be extremely helpful for a company that is curious about the public reaction to a new product. Rather than reading thousands of reviews or tweets about the product, you could feed those documents into the Naive Bayes classifier and instantly find out how many are positive and how many are negative.\n",
    "\n",
    "The dataset we will be using for this lesson contains Amazon product reviews for baby products. The original dataset contained many different features including the reviewer’s name, the date the review was made, and the overall score. We’ve removed many of those features; the only features that we’re interested in are the text of the review and whether the review was “positive” or “negative”. We labeled all reviews with a score less than 4 as a negative review.\n",
    "\n",
    "Note that in the next two lessons, we’ve only imported a small percentage of the data to help the code run faster. We’ll import the full dataset later when we put everything together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem I\n",
    "\n",
    "For the rest of this lesson, we’re going to write a classifier that can predict whether the review “This crib was amazing” is a positive or negative review.\n",
    "\n",
    "We want to compute both $P(positive | review)$ and $P(negative | review)$ and find which probability is larger. \n",
    "\n",
    "To do this, we’ll be using Bayes’ Theorem. Let’s look at Bayes’ Theorem for $P(positive | review)$.\n",
    "\n",
    "$P(\\text{positive}\\ | \\ \\text{review}) = \\frac{P(\\text{review | positive}) \\cdot P(\\text{positive})}{P(\\text{review})}$\n",
    "\n",
    "The first part of Bayes’ Theorem that we are going to tackle is P(positive). This is the probability that any review is positive. To find this, we need to look at all of our reviews in our dataset - both positive and negative - and find the percentage of reviews that are positive.\n",
    "\n",
    "We’ve bolded the part of Bayes’ Theorem we’re working on.\n",
    "\n",
    "$P(\\text{positive}\\ | \\ \\text{review}) = \\frac{P(\\text{review | positive}) \\cdot \\textbf{P(positive})}{P(\\text{review})}$\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem II\n",
    "\n",
    "Let’s continue to try to classify the review “This crib was amazing”.\n",
    "\n",
    "The second part of Bayes’ Theorem is a bit more extensive. We now want to compute P(review | positive).\n",
    "\n",
    "$P(\\text{positive}\\ | \\ \\text{review}) = \\frac{\\textbf{P(review | positive)} \\cdot P(\\text{positive})}{P(\\text{review})}$\n",
    " \n",
    "In other words, if we assume that the review is positive, what is the probability that the words “This”, “crib”, “was”, and “amazing” are the only words in the review?\n",
    "\n",
    "To find this, we have to assume that each word is conditionally independent. This means that one word appearing doesn’t affect the probability of another word from showing up. This is a pretty big assumption!\n",
    "\n",
    "We now have this equation. You can scroll to the right to see the full equation.\n",
    "\n",
    "$P(\\text{\"This crib was amazing\"} \\ |\\ \\text{positive}) = P(\\text{\"This\"}\\ |\\ \\text{positive})\\cdot P(\\text{\"crib\"}\\ |\\ \\text{positive})\\cdot P(\\text{\"was\"}\\ |\\ \\text{positive})\\cdot P(\\text{\"amazing\"}\\ |\\ \\text{positive})$\n",
    "\n",
    "Let’s break this down even further by looking at one of these terms. P(\"crib\"|positive) is the probability that the word “crib” appears in a positive review. To find this, we need to count up the total number of times “crib” appeared in our dataset of positive reviews. If we take that number and divide it by the total number of words in our positive review dataset, we will end up with the probability of “crib” appearing in a positive review.\n",
    "\n",
    "$P(\\text{\"crib\"}\\ |\\ \\text{positive}) = \\frac{\\text{# of \"crib\" in positive}}{\\text{# of words in positive}}$\n",
    "\t \n",
    "If we do this for every word in our review and multiply the results together, we have P(review | positive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "Let’s first find the total number of words in all positive reviews and store that number in a variable named total_pos.\n",
    "\n",
    "To do this, we can use the built-in Python sum() function. sum() takes a list as a parameter. The list that you want to sum is the values of the dictionary pos_counter, which you can get by using pos_counter.values().\n",
    "\n",
    "Do the same for total_neg.\n",
    "\n",
    "\n",
    "2.\n",
    "Create two variables named pos_probability and neg_probability. Each of these variables should start at 1. These are the variables we are going to use to keep track of the probabilities.\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Create a list of the words in review and store it in a variable named review_words. You can do this by using Python’s .split() function.\n",
    "\n",
    "For example if the string test contained \"Hello there\", then test.split() would return [\"Hello\", \"there\"].\n",
    "\n",
    "\n",
    "4.\n",
    "Loop through every word in review_words. Find the number of times word appears in pos_counter and neg_counter. Store those values in variables named word_in_pos and word_in_neg.\n",
    "\n",
    "In the next steps, we’ll use this variable inside the for loop to do a series of multiplications.\n",
    "\n",
    "\n",
    "Stuck? Get a hint\n",
    "5.\n",
    "Inside the for loop, set pos_probability to be pos_probability multiplied by word_in_pos / total_pos.\n",
    "\n",
    "This step is finding each term to be multiplied together. For example, when word is \"crib\", you’re calculating the following:\n",
    "\n",
    "$P(\\text{``crib\"}\\ |\\ \\text{positive}) = \\frac{\\text{\\# of ``crib\" in positive}}{\\text{\\# of words in positive}}$\n",
    " \n",
    "\n",
    "\n",
    "6.\n",
    "Do the same multiplication for neg_probability.\n",
    "\n",
    "Outside the for loop, print both pos_probability and neg_probability. Those values are P(“This crib was amazing”|positive) and P(“This crib was amazing”|negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-398c86f3ff18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpercent_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtotal_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtotal_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_counter' is not defined"
     ]
    }
   ],
   "source": [
    "#from reviews import neg_counter, pos_counter\n",
    "\n",
    "review = \"This crib was amazing\"\n",
    "\n",
    "percent_pos = 0.5\n",
    "percent_neg = 0.5\n",
    "\n",
    "total_pos = sum(pos_counter.values())\n",
    "total_neg = sum(neg_counter.values())\n",
    "\n",
    "pos_probability = 1\n",
    "neg_probability = 1\n",
    "\n",
    "review_words = review.split()\n",
    "\n",
    "for word in review_words:\n",
    "  word_in_pos = pos_counter[word]\n",
    "\n",
    " \n",
    "  word_in_neg = neg_counter[word]\n",
    "  \n",
    "  pos_probability *= word_in_pos / total_pos\n",
    "  neg_probability *= word_in_neg / total_neg\n",
    "  \n",
    "print(pos_probability)\n",
    "print(neg_probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing\n",
    "In the last exercise, one of the probabilities that we computed was the following:\n",
    "\n",
    "$P(\\text{``crib\"}\\ |\\ \\text{positive}) = \\frac{\\text{\\# of ``crib\" in positive}}{\\text{\\# of words in positive}}$\n",
    "\t \n",
    "But what happens if “crib” was never in any of the positive reviews in our dataset? This fraction would then be 0, and since everything is multiplied together, the entire probability P(review | positive) would become 0.\n",
    "\n",
    "This is especially problematic if there are typos in the review we are trying to classify. If the unclassified review has a typo in it, it is very unlikely that that same exact typo will be in the dataset, and the entire probability will be 0. To solve this problem, we will use a technique called smoothing.\n",
    "\n",
    "In this case, we smooth by adding 1 to the numerator of each probability and N to the denominator of each probability. N is the number of unique words in our review dataset.\n",
    "\n",
    "For example, P(\"crib\" | positive) goes from this:\n",
    "\n",
    "$P(\\text{\"crib\"}\\ |\\ \\text{positive}) = \\frac{\\text{\\# of \"crib\" in positive}}{\\text{\\# of words in positive}}$\n",
    "To this:\n",
    "\n",
    "$P(\\text{\"crib\"}\\ |\\ \\text{positive}) = \\frac{\\text{\\# of \"crib\" in positive} + 1}{\\text{\\# of words in positive}+ N}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "Let’s demonstrate how these probabilities break if there’s a word that never appears in the given datasets.\n",
    "\n",
    "Change review to \"This cribb was amazing\". Notice the second b in cribb.\n",
    "\n",
    "2.\n",
    "Inside your for loop, when you multiply pos_probability and neg_probability by a fraction, add 1 to the numerator.\n",
    "\n",
    "Make sure to include parentheses around the numerator!\n",
    "\n",
    "\n",
    "3.\n",
    "In the denominator of those fractions, add the number of unique words in the appropriate dataset.\n",
    "\n",
    "For the positive probability, this should be the length of pos_counter which can be found using len().\n",
    "\n",
    "Again, make sure to put parentheses around your denominator so the division happens after the addition!\n",
    "\n",
    "Did smoothing fix the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-48dcda06f94e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mreviews\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mneg_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"This cribb was amazing\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpercent_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'reviews'"
     ]
    }
   ],
   "source": [
    "from reviews import neg_counter, pos_counter\n",
    "\n",
    "review = \"This cribb was amazing\"\n",
    "\n",
    "percent_pos = 0.5\n",
    "percent_neg = 0.5\n",
    "\n",
    "total_pos = sum(pos_counter.values())\n",
    "total_neg = sum(neg_counter.values())\n",
    "\n",
    "pos_probability = 1\n",
    "neg_probability = 1\n",
    "\n",
    "review_words = review.split()\n",
    "\n",
    "for word in review_words:\n",
    "  word_in_pos = pos_counter[word]\n",
    "  word_in_neg = neg_counter[word]\n",
    "  \n",
    "  pos_probability *= (word_in_pos+1) / (total_pos+ len(pos_counter))\n",
    "  neg_probability *= (word_in_neg+1) / (total_neg+ len(neg_counter))\n",
    "  \n",
    "print(pos_probability)\n",
    "print(neg_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify\n",
    "If we look back to Bayes’ Theorem, we’ve now completed both parts of the numerator. We now need to multiply them together.\n",
    "\n",
    "$P(\\text{positive}\\ | \\ \\text{review}) = \\frac{\\textbf{P(review\\ |\\ positive)} \\cdot \\textbf{P(positive})}{P(\\text{review})}$\n",
    "\n",
    "Let’s now consider the denominator P(review). In our small example, this is the probability that “This”, “crib”, “was”, and “amazing” are the only words in the review. Notice that this is extremely similar to $P(review | positive)$. The only difference is that we don’t assume that the review is positive.\n",
    "\n",
    "However, before we start to compute the denominator, let’s think about what our ultimate question is. We want to predict whether the review “This crib was amazing” is a positive or negative review. In other words, we’re asking whether $P(positive | review)$ is greater than P(negative | review). If we expand those two probabilities, we end up with the following equations.\n",
    "\n",
    "$P(\\text{positive}\\ | \\ \\text{review}) = \\frac{P(\\text{review | positive})\\cdot \\text{P(positive})}{P(\\text{review})}$\n",
    "\n",
    "$P(\\text{negative}\\ | \\ \\text{review}) = \\frac{P(\\text{review | negative}) \\cdot \\text{P(negative})}{P(\\text{review})}$\n",
    "\n",
    "Notice that P(review) is in the denominator of each. That value will be the same in both cases! Since we’re only interested in comparing these two probabilities, there’s no reason why we need to divide them by the same value. We can completely ignore the denominator!\n",
    "\n",
    "Let’s see if our review was more likely to be positive or negative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "After the for loop, multiply pos_probability by percent_pos and neg_probability by percent_neg. Store the two values in final_pos and final_neg and print both.\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "Compare final_pos to final_neg:\n",
    "\n",
    "If final_pos was greater than final_neg, print \"The review is positive\".\n",
    "Otherwise print \"The review is negative\".\n",
    "Did our Naive Bayes Classifier get it right for the review \"This crib was amazing\"?\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Replace the review \"This crib was amazing\" with one that you think should be classified as negative. Run your program again.\n",
    "\n",
    "Did your classifier correctly classify the new review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-54960be8cbf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mreviews\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mneg_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"This crib was most of the case uncomfortable\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpercent_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'reviews'"
     ]
    }
   ],
   "source": [
    "from reviews import neg_counter, pos_counter\n",
    "\n",
    "review = \"This crib was most of the case uncomfortable\"\n",
    "\n",
    "percent_pos = 0.5\n",
    "percent_neg = 0.5\n",
    "\n",
    "total_pos = sum(pos_counter.values())\n",
    "total_neg = sum(neg_counter.values())\n",
    "\n",
    "pos_probability = 1\n",
    "neg_probability = 1\n",
    "\n",
    "review_words = review.split()\n",
    "\n",
    "for word in review_words:\n",
    "  word_in_pos = pos_counter[word]\n",
    "  word_in_neg = neg_counter[word]\n",
    "  \n",
    "  pos_probability *= (word_in_pos + 1) / (total_pos + len(pos_counter))\n",
    "  neg_probability *= (word_in_neg + 1) / (total_neg + len(neg_counter))\n",
    "\n",
    "final_pos = pos_probability * percent_pos\n",
    "final_neg = neg_probability * percent_neg\n",
    "\n",
    "\n",
    "if final_pos > final_neg:\n",
    "  print(\"The review is positive\")\n",
    "else:\n",
    "  print(\"The review is negative\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting the Data for scikit-learn\n",
    "\n",
    "Congratulations! You’ve made your own Naive Bayes text classifier. If you have a dataset of text that has been tagged with different classes, you can give your classifier a brand new document and it will predict what class it belongs to.\n",
    "\n",
    "We’re now going to look at how Python’s scikit-learn library can do all of that work for us!\n",
    "\n",
    "In order to use scikit-learn’s Naive Bayes classifier, we need to first transform our data into a format that scikit-learn can use. To do so, we’re going to use scikit-learn’s CountVectorizer object.\n",
    "\n",
    "To begin, we need to create a CountVectorizer and teach it the vocabulary of the training set. This is done by calling the .fit() method.\n",
    "\n",
    "For example, in the code below, we’ve created a CountVectorizer that has been trained on the vocabulary \"Training\", \"review\", \"one\", and \"Second\".\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit([\"Training review one\", \"Second review\"])\n",
    "\n",
    "After fitting the vectorizer, we can now call its .transform() method. The .transform() method takes a list of strings and will transform those strings into counts of the trained words. Take a look at the code below.\n",
    "\n",
    "counts = vectorizer.transform([\"one review two review\"])\n",
    "\n",
    "counts now stores the array [2, 1, 0, 0]. The word \"review\" appeared twice, the word \"one\" appeared once, and neither \"Training\" nor \"Second\" appeared at all.\n",
    "\n",
    "But how did we know that the 2 corresponded to review? You can print vectorizer.vocabulary_ to see the index that each word corresponds to. It might look something like this:\n",
    "\n",
    "{'one': 1, 'Training': 2, 'review': 0, 'Second': 3}\n",
    "\n",
    "Finally, notice that even though the word \"two\" was in our new review, there wasn’t an index for it in the vocabulary. This is because \"two\" wasn’t in any of the strings used in the .fit() method.\n",
    "\n",
    "We can now usecounts as input to our Naive Bayes Classifier.\n",
    "\n",
    "Note that in the code in the editor, we’ve imported only a small percentage of our review dataset to make load times faster. We’ll import the full dataset later when we put all of the pieces together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "Create a CountVectorizer and name it counter.\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "Call counter‘s .fit() method. .fit() takes a list of strings and it will learn the vocabulary of those strings. We want our counter to learn the vocabulary from both neg_list and pos_list.\n",
    "\n",
    "Call .fit() using neg_list + pos_list as a parameter.\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Print counter.vocabulary_. This is the vocabulary that your counter just learned. The numbers associated with each word are the indices of each word when you transform a review.\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "Let’s transform our brand new review. Create a variable named review_counts and set it equal to counter‘s .transform() function. Remember, .transform() takes a list of strings to transform. So call .transform() using [review] as a parameter.\n",
    "\n",
    "Print review_counts.toarray(). If you don’t include the toarray(), review_counts won’t print in a readable format.\n",
    "\n",
    "It looks like this is an array of all 0s, but the indices that correspond to the words \"this\", \"crib\", \"was\", and \"amazing\" should all be 1.\n",
    "\n",
    "\n",
    "\n",
    "5.\n",
    "We’ll use review_counts as the test point for our Naive Bayes Classifier, but we also need to transform our training set.\n",
    "\n",
    "Our training set is neg_list + pos_list. Call .transform() using that as a parameter. Store the results in a variable named training_counts. We’ll use these variables in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-335358c7bf11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mreviews\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mneg_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"This crib was amazing\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'reviews'"
     ]
    }
   ],
   "source": [
    "from reviews import neg_list, pos_list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "review = \"This crib was amazing\"\n",
    "\n",
    "counter = CountVectorizer()\n",
    "counter.fit(neg_list + pos_list)\n",
    "print(counter.vocabulary_)\n",
    "\n",
    "review_counts = counter.transform([review])\n",
    "print(review_counts.toarray())\n",
    "\n",
    "training_counts = counter.transform(neg_list + pos_list)\n",
    "\n",
    "print(training_counts.toarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using scikit-learn\n",
    "Now that we’ve formatted our data correctly, we can use it using scikit-learn’s MultinomialNB classifier.\n",
    "\n",
    "This classifier can be trained using the .fit() method. .fit() takes two parameters: The array of data points (which we just made) and an array of labels corresponding to each data point.\n",
    "\n",
    "Finally, once the model has been trained, we can use the .predict() method to predict the labels of new points. .predict() takes a list of points that you want to classify and it returns the predicted labels of those points.\n",
    "\n",
    "Finally, .predict_proba() will return the probability of each label given a point. Instead of just returning whether the review was good or bad, it will return the likelihood of a good or bad review.\n",
    "\n",
    "Note that in the code editor, we’ve imported some of the variables you created last time. Specifically, we’ve imported the counter object, training_counts and then make review_counts. This means the program won’t have to re-create those variables and should help the runtime of your program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "Begin by making a MultinomialNB object called classifier.\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "We now want to fit the classifier. We have the transformed points (found in training_counts), but we don’t have the labels associated with those points.\n",
    "\n",
    "We made the training points by combining neg_list and pos_list. So the first half of the labels should be 0 (for negative) and the second half should be 1 (for positive).\n",
    "\n",
    "Create a list named training_labels that has 1000 0s followed by 1000 1s.\n",
    "\n",
    "Note that there are 1000 negative and 1000 positive reviews. Normally you could find this out by asking for the length of your dataset — in this example, we haven’t included the dataset because it takes so long to load!\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Call classifier‘s .fit() function. Fit takes two parameters: the training set and the training labels.\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "Call classifier‘s .predict() method and print the results. This method takes a list of the points that you want to test.\n",
    "\n",
    "Was your review classified as a positive or negative review?\n",
    "\n",
    "\n",
    "5.\n",
    "After printing predict, print a call to the predict_proba method. The parameter to predict_proba should be the same as predict.\n",
    "\n",
    "The first number printed is the probability that the review was a 0 (bad) and the second number is the probability the review was a 1 (good).\n",
    "\n",
    "6.\n",
    "Change the text review to see the probabilities change.\n",
    "\n",
    "Can you create a review that the algorithm is really confident about being positive?\n",
    "\n",
    "The review \"This crib was great amazing and wonderful\" had the following probabilities:\n",
    "\n",
    "[[ 0.04977729 0.95022271]]\n",
    "\n",
    "Can you create a review that is even more positive?\n",
    "\n",
    "Another interesting challenge is to create a clearly negative review that our classifier thinks is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b6a06eb8c9fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mreviews\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"This crib was great amazing and wonderful\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'reviews'"
     ]
    }
   ],
   "source": [
    "from reviews import counter, training_counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "review = \"This crib was great amazing and wonderful\"\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "training_labels = [0] * 1000 + [1] * 1000\n",
    "\n",
    "classifier.fit(training_counts, training_labels)\n",
    "\n",
    "print(classifier.predict(review_counts))\n",
    "\n",
    "print(classifier.predict_proba(review_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "\n",
    "In this lesson, you’ve learned how to leverage Bayes’ Theorem to create a supervised machine learning algorithm. Here are some of the major takeaways from the lesson:\n",
    "\n",
    "* A tagged dataset is necessary to calculate the probabilities used in Bayes’ Theorem.\n",
    "\n",
    "\n",
    "* In this example, the features of our dataset are the words used in a product review. In order to apply Bayes’ Theorem, we assume that these features are independent.\n",
    "\n",
    "\n",
    "* Using Bayes’ Theorem, we can find P(class|data point) for every possible class. In this example, there were two classes — positive and negative. The class with the highest probability will be the algorithm’s prediction.\n",
    "\n",
    "\n",
    "Even though our algorithm is running smoothly, there’s always more that we can add to try to improve performance. The following techniques are focused on ways in which we process data before feeding it into the Naive Bayes classifier:\n",
    "\n",
    "* Remove punctuation from the training set. Right now in our dataset, there are 702 instances of \"great!\" and 2322 instances of \"great.\". We should probably combine those into 3024 instances of \"great\".\n",
    "\n",
    "\n",
    "* Lowercase every word in the training set. We do this for the same reason why we remove punctuation. We want \"Great\" and \"great\" to be the same.\n",
    "\n",
    "\n",
    "* Use a bigram or trigram model. Right now, the features of a review are individual words. For example, the features of the point “This crib is great” are “This”, “crib”, “is”, and “great”. If we used a bigram model, the features would be “This crib”, “crib is”, and “is great”. Using a bigram model makes the assumption of independence more reasonable.\n",
    "\n",
    "These three improvements would all be considered part of the field Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "we’ve included three Naive Bayes classifiers that have been trained on different datasets. The training sets used are the baby product reviews, reviews for Amazon Instant Videos, and reviews about video games.\n",
    "\n",
    "Try changing review again and see how the different classifiers react!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5925cb798f9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mreviews\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbaby_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaby_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstant_video_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstant_video_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_game_counter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_game_training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"this game was violent\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'reviews'"
     ]
    }
   ],
   "source": [
    "from reviews import baby_counter, baby_training, instant_video_counter, instant_video_training, video_game_counter, video_game_training\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "review = \"this game was violent\"\n",
    "\n",
    "baby_review_counts = baby_counter.transform([review])\n",
    "instant_video_review_counts = instant_video_counter.transform([review])\n",
    "video_game_review_counts = video_game_counter.transform([review])\n",
    "\n",
    "baby_classifier = MultinomialNB()\n",
    "instant_video_classifier = MultinomialNB()\n",
    "video_game_classifier = MultinomialNB()\n",
    "\n",
    "baby_labels = [0] * 1000 + [1] * 1000\n",
    "instant_video_labels = [0] * 1000 + [1] * 1000\n",
    "video_game_labels = [0] * 1000 + [1] * 1000\n",
    "\n",
    "\n",
    "baby_classifier.fit(baby_training, baby_labels)\n",
    "instant_video_classifier.fit(instant_video_training, instant_video_labels)\n",
    "video_game_classifier.fit(video_game_training, video_game_labels)\n",
    "\n",
    "print(\"Baby training set: \" +str(baby_classifier.predict_proba(baby_review_counts)))\n",
    "\n",
    "print(baby_classifier.predict(baby_review_counts))\n",
    "print('')\n",
    "\n",
    "print(\"Amazon Instant Video training set: \" + str(instant_video_classifier.predict_proba(instant_video_review_counts)))\n",
    "\n",
    "print(instant_video_classifier.predict(instant_video_review_counts))\n",
    "print('')\n",
    "\n",
    "print(\"Video Games training set: \" + str(video_game_classifier.predict_proba(video_game_review_counts)))\n",
    "\n",
    "print(video_game_classifier.predict(video_game_review_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
