{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NEAREST NEIGHBORS\n",
    "## Introduction\n",
    "Before diving into the K-Nearest Neighbors algorithm, let’s first take a minute to think about an example.\n",
    "\n",
    "Consider a dataset of movies. Let’s brainstorm some features of a movie data point. A feature is a piece of information associated with a data point. Here are some potential features of movie data points:\n",
    "\n",
    "* the length of the movie in minutes.\n",
    "* the budget of a movie in dollars.\n",
    "\n",
    "If you think back to the previous exercise, you could imagine movies being places in that two-dimensional space based on those numeric features. There could also be some boolean features: features that are either true or false. For example, here are some potential boolean features:\n",
    "\n",
    "* Black and white. This feature would be True for black and white movies and False otherwise.\n",
    "* Directed by Stanley Kubrick. This feature would be False for almost every movie, but for the few movies that were directed by Kubrick, it would be True.\n",
    "\n",
    "Finally, let’s think about how we might want to classify a movie. For the rest of this lesson, we’re going to be classifying movies as either good or bad. In our dataset, we’ve classified a movie as good if it had an IMDb rating of 7.0 or greater. Every “good” movie will have a class of 1, while every bad movie will have a class of 0.\n",
    "\n",
    "To the right, we’ve created some movie data points where the first item in the list is the length, the second is the budget, and the third is whether the movie was directed by Stanley Kubrick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:\n",
    "Add the variable gone_with_the_wind. This movie is 238 minutes long (wow!), had a budget of $3,977,000, and was not directed by Stanley Kubrick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_girls = [97, 17000000, False]\n",
    "the_shining = [146, 19000000, True]\n",
    "\n",
    "gone_with_the_wind = [238, 3977000,False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Between Points - 2D\n",
    "\n",
    "In the first exercise, we were able to visualize the dataset and estimate the k nearest neighbors of an unknown point. But a computer isn’t going to be able to do that!\n",
    "\n",
    "We need to define what it means for two points to be close together or far apart. To do this, we’re going to use the Distance Formula.\n",
    "\n",
    "For this example, the data has two dimensions:\n",
    "\n",
    "The length of the movie\n",
    "The movie’s release date\n",
    "Consider Star Wars and Raiders of the Lost Ark. Star Wars is 125 minutes long and was released in 1977. Raiders of the Lost Ark is 115 minutes long and was released in 1981.\n",
    "\n",
    "The distance between the movies is computed below:\n",
    "\n",
    "distance formula example\n",
    "<img src=\"images/StarWars-Raiders-Distance.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Write a function named distance that takes two lists named movie1 and movie2 as parameters.\n",
    "\n",
    "You can assume that each of these lists contains two numbers — the first number being the movie’s runtime and the second number being the year the movie was released. The function should return the distance between the two lists.\n",
    "\n",
    "Remember, in python, x ** 0.5 will give you the square root of x.\n",
    "\n",
    "Similarly, x ** 2 will give you the square of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.770329614269007\n",
      "38.897300677553446\n"
     ]
    }
   ],
   "source": [
    "star_wars = [125, 1977]\n",
    "raiders = [115, 1981]\n",
    "mean_girls = [97, 2004]\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    difference += (movie1[i] - movie2[i])**2\n",
    "  distance = difference**0.5\n",
    "  return distance\n",
    "  # length_difference = (movie1[0] - movie2[0]) ** 2\n",
    "  # year_difference = (movie1[1] - movie2[1]) ** 2\n",
    "  # distance = (length_difference + year_difference) ** 0.5\n",
    "  # return distance\n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance Between Points - 3D\n",
    "Making a movie rating predictor based on just the length and release date of movies is pretty limited. There are so many more interesting pieces of data about movies that we could use! So let’s add another dimension.\n",
    "\n",
    "Let’s say this third dimension is the movie’s budget. We now have to find the distance between these two points in three dimensions.\n",
    "<img src=\"images/threed.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "3D graph\n",
    "What if we’re not happy with just three dimensions? Unfortunately, it becomes pretty difficult to visualize points in dimensions higher than 3. But that doesn’t mean we can’t find the distance between them.\n",
    "\n",
    "The generalized distance formula between points A and B |is as follows:\n",
    "\n",
    "$ \\sqrt{(A_1-B_1)^2+(A_2-B_2)^2+ \\dots+(A_n-B_n)^2} $\n",
    "\t \n",
    "Here, A1-B1 is the difference between the first feature of each point. An-Bn is the difference between the last feature of each point.\n",
    "\n",
    "Using this formula, we can find the K-Nearest Neighbors of a point in N-dimensional space! We now can use as much information about our movies as we want.\n",
    "\n",
    "We will eventually use these distances to find the nearest neighbors to an unlabeled point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000000.000008286\n",
      "6000000.000126083\n"
     ]
    }
   ],
   "source": [
    "star_wars = [125, 1977, 11000000]\n",
    "raiders = [115, 1981, 18000000]\n",
    "mean_girls = [97, 2004, 17000000]\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data with Different Scales: Normalization\n",
    "In the next three lessons, we’ll implement the three steps of the K-Nearest Neighbor Algorithm:\n",
    "\n",
    "Normalize the data\n",
    "Find the k nearest neighbors\n",
    "Classify the new point based on those neighbors\n",
    "When we added the dimension of budget, you might have realized there are some problems with the way our data currently looks.\n",
    "\n",
    "Consider the two dimensions of release date and budget. The maximum difference between two movies’ release dates is about 125 years (The Lumière Brothers were making movies in the 1890s). However, the difference between two movies’ budget can be millions of dollars.\n",
    "\n",
    "The problem is that the distance formula treats all dimensions equally, regardless of their scale. If two movies came out 70 years apart, that should be a pretty big deal. However, right now, that’s exactly equivalent to two movies that have a difference in budget of 70 dollars. The difference in one year is exactly equal to the difference in one dollar of budget. That’s absurd!\n",
    "\n",
    "Another way of thinking about this is that the budget completely outweighs the importance of all other dimensions because it is on such a huge scale. The fact that two movies were 70 years apart is essentially meaningless compared to the difference in millions in the other dimension.\n",
    "\n",
    "The solution to this problem is to normalize the data so every value is between 0 and 1. In this lesson, we’re going to be using min-max normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047619047619047616, 0.8492063492063492, 0.8650793650793651, 0.4523809523809524, 0.5634920634920635, 0.46825396825396826, 0.6666666666666666, 0.5476190476190477, 1.0, 0.36507936507936506, 0.6111111111111112, 0.8333333333333334, 0.42063492063492064, 0.0, 0.8253968253968254, 0.4523809523809524, 0.9523809523809523, 0.5873015873015873, 0.0, 0.6904761904761905]\n"
     ]
    }
   ],
   "source": [
    "release_dates = [1897, 1998, 2000, 1948, 1962, 1950, 1975, 1960, 2017, 1937, 1968, 1996, 1944, 1891, 1995, 1948, 2011, 1965, 1891, 1978]\n",
    "\n",
    "\n",
    "def min_max_normalize(lst):\n",
    "  minimum =min(lst)\n",
    "  maximum =max(lst)\n",
    "  normalized = []\n",
    "  for i in range(len(lst)):\n",
    "    norm = (lst[i]- minimum)/(maximum-minimum)\n",
    "    normalized.append(norm)\n",
    "  return normalized\n",
    "\n",
    "print(min_max_normalize(release_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Nearest Neighbors\n",
    "The K-Nearest Neighbor Algorithm:\n",
    "\n",
    "Normalize the data\n",
    "Find the k nearest neighbors\n",
    "Classify the new point based on those neighbors\n",
    "Now that our data has been normalized and we know how to find the distance between two points, we can begin classifying unknown data!\n",
    "\n",
    "To do this, we want to find the k nearest neighbors of the unclassified point. In a few exercises, we’ll learn how to properly choose k, but for now, let’s choose a number that seems somewhat reasonable. Let’s choose 5.\n",
    "\n",
    "In order to find the 5 nearest neighbors, we need to compare this new unclassified movie to every other movie in the dataset. This means we’re going to be using the distance formula again and again. We ultimately want to end up with a sorted list of distances and the movies associated with those distances.\n",
    "\n",
    "It might look something like this:\n",
    "\n",
    "[\n",
    "  [0.30, 'Superman II'],\n",
    "  \n",
    "  [0.31, 'Finding Nemo'],\n",
    "  \n",
    "  ...\n",
    "  \n",
    "  ...\n",
    "  \n",
    "  [0.38, 'Blazing Saddles']\n",
    "]\n",
    "\n",
    "In this example, the unknown movie has a distance of 0.30 to Superman II.\n",
    "\n",
    "In the next exercise, we’ll use the labels associated with these movies to classify the unlabeled point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'movie_dataset' from 'movies' (C:\\Users\\naim_\\Anaconda3\\lib\\site-packages\\movies.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-75479674a41d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmovies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmovie_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovie_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(movie_dataset['Bruce Almighty'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(movie_labels['Bruce Almighty'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'movie_dataset' from 'movies' (C:\\Users\\naim_\\Anaconda3\\lib\\site-packages\\movies.py)"
     ]
    }
   ],
   "source": [
    "from movies import movie_dataset, movie_labels\n",
    "\n",
    "#print(movie_dataset['Bruce Almighty'])\n",
    "#print(movie_labels['Bruce Almighty'])\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort() \n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  return neighbors\n",
    "  \n",
    "print(classify([.4, .2, .9], movie_dataset, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Sets\n",
    "You’ve now built your first K Nearest Neighbors algorithm capable of classification. You can feed your program a never-before-seen movie and it can predict whether its IMDb rating was above or below 7.0. However, we’re not done yet. We now need to report how effective our algorithm is. After all, it’s possible our predictions are totally wrong!\n",
    "\n",
    "As with most machine learning algorithms, we have split our data into a training set and validation set.\n",
    "\n",
    "Once these sets are created, we will want to use every point in the validation set as input to the K Nearest Neighbor algorithm. We will take a movie from the validation set, compare it to all the movies in the training set, find the K Nearest Neighbors, and make a prediction. After making that prediction, we can then peek at the real answer (found in the validation labels) to see if our classifier got the answer correct.\n",
    "\n",
    "If we do this for every movie in the validation set, we can count the number of times the classifier got the answer right and the number of times it got it wrong. Using those two numbers, we can compute the validation accuracy.\n",
    "\n",
    "Validation accuracy will change depending on what K we use. In the next exercise, we’ll use the validation accuracy to pick the best possible K for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'training_set' from 'movies' (C:\\Users\\naim_\\Anaconda3\\lib\\site-packages\\movies.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2fffd257a146>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmovies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovie2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0msquared_difference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'training_set' from 'movies' (C:\\Users\\naim_\\Anaconda3\\lib\\site-packages\\movies.py)"
     ]
    }
   ],
   "source": [
    "from movies import training_set, training_labels, validation_set, validation_labels\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for neighbor in neighbors:\n",
    "    title = neighbor[1]\n",
    "    if labels[title] == 0:\n",
    "      num_bad += 1\n",
    "    elif labels[title] == 1:\n",
    "      num_good += 1\n",
    "  if num_good > num_bad:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "  \n",
    "print(validation_set[\"Bee Movie\"])\n",
    "print(validation_labels[\"Bee Movie\"])\n",
    "\n",
    "guess=classify(validation_set[\"Bee Movie\"], training_set, training_labels, 5)\n",
    "print(guess)\n",
    "if guess == validation_labels[\"Bee Movie\"]:\n",
    "  print(\"Correct!\")\n",
    "else:\n",
    "  print(\"Wrong!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing K\n",
    "In the previous exercise, we found that our classifier got one point in the training set correct. Now we can test every point to calculate the validation accuracy.\n",
    "\n",
    "The validation accuracy changes as k changes. The first situation that will be useful to consider is when k is very small. Let’s say k = 1. We would expect the validation accuracy to be fairly low due to overfitting. Overfitting is a concept that will appear almost any time you are writing a machine learning algorithm. Overfitting occurs when you rely too heavily on your training data; you assume that data in the real world will always behave exactly like your training data. In the case of K-Nearest Neighbors, overfitting happens when you don’t consider enough neighbors. A single outlier could drastically determine the label of an unknown point. Consider the image below.\n",
    "<img src=\"images/overfit.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "colored dots with a single outlier\n",
    "\n",
    "The dark blue point in the top left corner of the graph looks like a fairly significant outlier. When k = 1, all points in that general area will be classified as dark blue when it should probably be classified as green. Our classifier has relied too heavily on the small quirks in the training data.\n",
    "\n",
    "On the other hand, if k is very large, our classifier will suffer from underfitting. Underfitting occurs when your classifier doesn’t pay enough attention to the small quirks in the training set. Imagine you have 100 points in your training set and you set k = 100. Every single unknown point will be classified in the same exact way. The distances between the points don’t matter at all! This is an extreme example, however, it demonstrates how the classifier can lose understanding of the training data if k is too big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "1.\n",
    "Begin by creating a function called find_validation_accuracy that takes five parameters. The parameters should be training_set, training_labels, validation_set, validation_labels, and k.\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "Create a variable called num_correct and have it begin at 0.0. Loop through the movies of validation_set, and call classify using each movie’s data, the training_set, the training_labels, and k. Store the result in a variable called guess. For now, return guess outside of your loop.\n",
    "\n",
    "Remember, the movie’s data can be found by using validation_set[title].\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Inside the for loop, compare guess to the corresponding label in validation_labels. If they were equal, add 1 to num_correct. For now, outside of the for loop, return num_correct\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "Outside the for loop return the validation error. This should be num_correct divided by the total number of points in the validation set.\n",
    "\n",
    "\n",
    "\n",
    "5.\n",
    "Call find_validation_accuracy with k = 3. Print the results The code should take a couple of seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'training_set' from 'movies' (C:\\Users\\naim_\\Anaconda3\\lib\\site-packages\\movies.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-697ef71a939f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmovies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovie2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0msquared_difference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'training_set' from 'movies' (C:\\Users\\naim_\\Anaconda3\\lib\\site-packages\\movies.py)"
     ]
    }
   ],
   "source": [
    "from movies import training_set, training_labels, validation_set, validation_labels\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for neighbor in neighbors:\n",
    "    title = neighbor[1]\n",
    "    if labels[title] == 0:\n",
    "      num_bad += 1\n",
    "    elif labels[title] == 1:\n",
    "      num_good += 1\n",
    "  if num_good > num_bad:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def find_validation_accuracy(training_set, training_labels,validation_set, validation_labels, k):\n",
    "  num_correct = 0.0\n",
    "  for title in validation_set:\n",
    "    guess=classify(validation_set[title],training_set,training_labels, k)\n",
    "    if guess == validation_labels[title]:\n",
    "      num_correct+=1\n",
    "  num_correct /= len(validation_set)\n",
    "  \n",
    "  return num_correct\n",
    "print(find_validation_accuracy(training_set, training_labels,validation_set, validation_labels,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of K\n",
    "The graph to the right shows the validation accuracy of our movie classifier as k increases. When k is small, overfitting occurs and the accuracy is relatively low. On the other hand, when k gets too large, underfitting occurs and accuracy starts to drop. \n",
    "<img src=\"images/KnnGraph_Update_1.svg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-NEAREST NEIGHBORS\n",
    "## Using sklearn\n",
    "\n",
    "You’ve now written your own K-Nearest Neighbor classifier from scratch! However, rather than writing your own classifier every time, you can use Python’s sklearn library. sklearn is a Python library specifically used for Machine Learning. It has an amazing number of features, but for now, we’re only going to investigate its K-Nearest Neighbor classifier.\n",
    "\n",
    "There are a couple of steps we’ll need to go through in order to use the library. First, you need to create a KNeighborsClassifier object. This object takes one parameter - k. For example, the code below will create a classifier where k = 3\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors = 3)\n",
    "Next, we’ll need to train our classifier. The .fit() method takes two parameters. The first is a list of points, and the second is the labels associated with those points. So for our movie example, we might have something like this\n",
    "\n",
    "training_points = [\n",
    "  [0.5, 0.2, 0.1],\n",
    "  [0.9, 0.7, 0.3],\n",
    "  [0.4, 0.5, 0.7]\n",
    "]\n",
    "\n",
    "training_labels = [0, 1, 1]\n",
    "classifier.fit(training_points, training_labels)\n",
    "Finally, after training the model, we can classify new points. The .predict() method takes a list of points that you want to classify. It returns a list of its guesses for those points.\n",
    "\n",
    "unknown_points = [\n",
    "  [0.2, 0.1, 0.7],\n",
    "  [0.4, 0.7, 0.6],\n",
    "  [0.5, 0.8, 0.1]\n",
    "]\n",
    "\n",
    "guesses = classifier.predict(unknown_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'movie_dataset' from 'movies' (C:\\Users\\naim_\\Anaconda3\\lib\\site-packages\\movies.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1704d8b06f2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmovies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmovie_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'movie_dataset' from 'movies' (C:\\Users\\naim_\\Anaconda3\\lib\\site-packages\\movies.py)"
     ]
    }
   ],
   "source": [
    "from movies import movie_dataset, labels\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "classifier.fit(movie_dataset, labels)\n",
    "\n",
    "unknown_movies = [[.45, .2, .5], [.25, .8, .9],[.1, .1, .9]]\n",
    "guess = classifier.predict(unknown_movies)\n",
    "\n",
    "print(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "Congratulations! You just implemented your very own classifier from scratch and used Python’s sklearn library. In this lesson, you learned some techniques very specific to the K-Nearest Neighbor algorithm, but some general machine learning techniques as well. Some of the major takeaways from this lesson include:\n",
    "\n",
    "* Data with n features can be conceptualized as points lying in n-dimensional space.\n",
    "\n",
    "* Data points can be compared by using the distance formula. Data points that are similar will have a smaller distance between them.\n",
    "\n",
    "* A point with an unknown class can be classified by finding the k nearest neighbors\n",
    "\n",
    "* To verify the effectiveness of a classifier, data with known classes can be split into a training set and a validation set. Validation error can then be calculated.\n",
    "\n",
    "* Classifiers have parameters that can be tuned to increase their effectiveness. In the case of K-Nearest Neighbors, k can be changed. \n",
    "\n",
    "* A classifier can be trained improperly and suffer from overfitting or underfitting. In the case of K-Nearest Neighbors, a low k often leads to overfitting and a large k often leads to underfitting.\n",
    "\n",
    "* Python’s sklearn library can be used for many classification and machine learning algorithms.\n",
    "\n",
    "To the right is an interactive visualization of K-Nearest Neighbors. If you move your mouse over the canvas, the location of your mouse will be classified as either green or blue. The nearest neighbors to your mouse are highlighted in yellow. Use the slider to change k to see how the boundaries of the classification change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
