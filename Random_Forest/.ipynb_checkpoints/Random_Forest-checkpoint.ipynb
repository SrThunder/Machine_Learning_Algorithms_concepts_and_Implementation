{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "We’ve seen that decision trees can be powerful supervised machine learning models. However, they’re not without their weaknesses — decision trees are often prone to overfitting.\n",
    "\n",
    "We’ve discussed some strategies to minimize this problem, like pruning, but sometimes that isn’t enough. We need to find another way to generalize our trees. This is where the concept of a random forest comes in handy.\n",
    "\n",
    "A random forest is an ensemble machine learning technique — a random forest contains many decision trees that all work together to classify new points. When a random forest is asked to classify a new point, the random forest gives that point to each of the decision trees. Each of those trees reports their classification and the random forest returns the most popular classification. It’s like every tree gets a vote, and the most popular classification wins.\n",
    "\n",
    "Some of the trees in the random forest may be overfit, but by making the prediction based on a large number of trees, overfitting will have less of an impact.\n",
    "\n",
    "In this lesson, we’ll learn how the trees in a random forest get created.\n",
    "\n",
    "<img src=\"images/random_forest.png\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "You might be wondering how the trees in the random forest get created. After all, right now, our algorithm for creating a decision tree is deterministic — given a training set, the same tree will be made every time.\n",
    "\n",
    "Random forests create different trees using a process known as bagging. Every time a decision tree is made, it is created using a different subset of the points in the training set. For example, if our training set had 1000 rows in it, we could make a decision tree by picking 100 of those rows at random to build the tree. This way, every tree is different, but all trees will still be created from a portion of the training data.\n",
    "\n",
    "One thing to note is that when we’re randomly selecting these 100 rows, we’re doing so with replacement. Picture putting all 100 rows in a bag and reaching in and grabbing one row at random. After writing down what row we picked, we put that row back in our bag.\n",
    "\n",
    "This means that when we’re picking our 100 random rows, we could pick the same row more than once. In fact, it’s very unlikely, but all 100 randomly picked rows could all be the same row!\n",
    "\n",
    "Because we’re picking these rows with replacement, there’s no need to shrink our bagged training set from 1000 rows to 100. We can pick 1000 rows at random, and because we can get the same row more than once, we’ll still end up with a unique data set.\n",
    "\n",
    "Let’s implement bagging! We’ll be using the data set of cars that we used in our decision tree lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "def make_cars():\n",
    "    f = open(\"car.csv\", \"r\")\n",
    "    cars = []\n",
    "    for line in f:\n",
    "        cars.append(line.rstrip().split(\",\"))\n",
    "    return cars\n",
    "  \n",
    "cars = make_cars()\n",
    "random.shuffle(cars)\n",
    "cars = cars[:1000]\n",
    "car_data = [x[:-1] for x in cars]\n",
    "car_labels = [x[-1] for x in cars]\n",
    "\n",
    "def split(dataset, labels, column):\n",
    "    data_subsets = []\n",
    "    label_subsets = []\n",
    "    counts = list(set([data[column] for data in dataset]))\n",
    "    counts.sort()\n",
    "    for k in counts:\n",
    "        new_data_subset = []\n",
    "        new_label_subset = []\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset[i][column] == k:\n",
    "                new_data_subset.append(dataset[i])\n",
    "                new_label_subset.append(labels[i])\n",
    "        data_subsets.append(new_data_subset)\n",
    "        label_subsets.append(new_label_subset)\n",
    "    return data_subsets, label_subsets\n",
    "\n",
    "def gini(dataset):\n",
    "  impurity = 1\n",
    "  label_counts = Counter(dataset)\n",
    "  for label in label_counts:\n",
    "    prob_of_label = label_counts[label] / len(dataset)\n",
    "    impurity -= prob_of_label ** 2\n",
    "  return impurity\n",
    "\n",
    "def information_gain(starting_labels, split_labels):\n",
    "  info_gain = gini(starting_labels)\n",
    "  for subset in split_labels:\n",
    "    info_gain -= gini(subset) * len(subset)/len(starting_labels)\n",
    "  return info_gain  \n",
    "  \n",
    "class Leaf:\n",
    "\n",
    "    def __init__(self, labels, value):\n",
    "        self.predictions = Counter(labels)\n",
    "        self.value = value\n",
    "\n",
    "class Decision_Node:\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 branches, value):\n",
    "        self.question = question\n",
    "        self.branches = branches\n",
    "        self.value = value\n",
    "  \n",
    "def print_tree(node, spacing=\"\"):\n",
    "    \"\"\"World's most elegant tree printing function.\"\"\"\n",
    "    question_dict = {0: \"Buying Price\", 1:\"Price of maintenance\", 2:\"Number of doors\", 3:\"Person Capacity\", 4:\"Size of luggage boot\", 5:\"Estimated Saftey\"}\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + question_dict[node.question])\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    for i in range(len(node.branches)):\n",
    "        print (spacing + '--> Branch ' + node.branches[i].value+':')\n",
    "        print_tree(node.branches[i], spacing + \"  \")\n",
    "        \n",
    "def find_best_split(dataset, labels):\n",
    "    best_gain = 0\n",
    "    best_feature = 0\n",
    "    for feature in range(len(dataset[0])):\n",
    "        data_subsets, label_subsets = split(dataset, labels, feature)\n",
    "        gain = information_gain(labels, label_subsets)\n",
    "        if gain > best_gain:\n",
    "            best_gain, best_feature = gain, feature\n",
    "    return best_gain, best_feature\n",
    "  \n",
    "def build_tree(rows, labels, value = \"\"):\n",
    "    gain, question = find_best_split(rows, labels)\n",
    "    if gain == 0:\n",
    "        return Leaf(labels, value)\n",
    "    data_subsets, label_subsets = split(rows, labels, question)\n",
    "    branches = []\n",
    "    for i in range(len(data_subsets)):\n",
    "        branch = build_tree(data_subsets[i], label_subsets[i], data_subsets[i][0][question])\n",
    "        branches.append(branch)\n",
    "    return Decision_Node(question, branches, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Start by creating a tree using all of the data we’ve given you. Create a variable named tree and set it equal to the build_tree() function using car_data and car_labels as parameters.\n",
    "\n",
    "Then call print_tree() using tree as a parameter. Scroll up to the top to see the root of the tree. Which feature is used to split the data at the root?\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "For now, comment out printing the tree.\n",
    "\n",
    "Let’s now implement bagging. The original dataset has 1000 items in it. We want to randomly select a subset of those with replacement.\n",
    "\n",
    "Create a list named indices that contains 1000 random numbers between 0 and 1000. We’ll use this list to remember the 1000 cars and the 1000 labels that we’re going to build a tree with.\n",
    "\n",
    "You can use either a for loop or list comprehension to make this list. To get a random number between 0 and 1000, use random.randint(0, 999).\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Create two new lists named data_subset and labels_subset. These two lists should contain the cars and labels found at each index in indices.\n",
    "\n",
    "Once again, you can use either a for loop or list comprehension to make these lists.\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "Create a tree named subset_tree using the build_tree() function with data_subset and labels_subset as parameters.\n",
    "\n",
    "Print subset_tree using the print_tree() function.\n",
    "\n",
    "Which feature is used to split the data at the root? Is it a different feature than the feature that split the tree that was created using all of the data?\n",
    "\n",
    "You’ve just created a new tree from the training set! If you used 1000 different indices, you’d get another different tree. You could now create a random forest by creating multiple different trees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person Capacity\n",
      "--> Branch 2:\n",
      "  Predict Counter({'unacc': 354})\n",
      "--> Branch 4:\n",
      "  Estimated Saftey\n",
      "  --> Branch high:\n",
      "    Buying Price\n",
      "    --> Branch high:\n",
      "      Price of maintenance\n",
      "      --> Branch high:\n",
      "        Predict Counter({'acc': 3})\n",
      "      --> Branch low:\n",
      "        Predict Counter({'acc': 6})\n",
      "      --> Branch med:\n",
      "        Predict Counter({'acc': 6})\n",
      "      --> Branch vhigh:\n",
      "        Predict Counter({'unacc': 6})\n",
      "    --> Branch low:\n",
      "      Price of maintenance\n",
      "      --> Branch high:\n",
      "        Size of luggage boot\n",
      "        --> Branch big:\n",
      "          Predict Counter({'vgood': 6})\n",
      "        --> Branch med:\n",
      "          Number of doors\n",
      "          --> Branch 2:\n",
      "            Predict Counter({'acc': 1})\n",
      "          --> Branch 5more:\n",
      "            Predict Counter({'vgood': 2})\n",
      "        --> Branch small:\n",
      "          Predict Counter({'acc': 1})\n",
      "      --> Branch low:\n",
      "        Size of luggage boot\n",
      "        --> Branch big:\n",
      "          Predict Counter({'vgood': 2})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'vgood': 4})\n",
      "        --> Branch small:\n",
      "          Predict Counter({'good': 2})\n",
      "      --> Branch med:\n",
      "        Number of doors\n",
      "        --> Branch 2:\n",
      "          Predict Counter({'vgood': 1})\n",
      "        --> Branch 3:\n",
      "          Size of luggage boot\n",
      "          --> Branch big:\n",
      "            Predict Counter({'vgood': 2})\n",
      "          --> Branch med:\n",
      "            Predict Counter({'good': 3})\n",
      "        --> Branch 4:\n",
      "          Predict Counter({'vgood': 5})\n",
      "        --> Branch 5more:\n",
      "          Predict Counter({'vgood': 2})\n",
      "      --> Branch vhigh:\n",
      "        Predict Counter({'acc': 6})\n",
      "    --> Branch med:\n",
      "      Price of maintenance\n",
      "      --> Branch high:\n",
      "        Predict Counter({'acc': 5})\n",
      "      --> Branch low:\n",
      "        Size of luggage boot\n",
      "        --> Branch big:\n",
      "          Predict Counter({'vgood': 1})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'good': 2})\n",
      "        --> Branch small:\n",
      "          Predict Counter({'good': 5})\n",
      "      --> Branch med:\n",
      "        Number of doors\n",
      "        --> Branch 3:\n",
      "          Predict Counter({'acc': 1})\n",
      "        --> Branch 5more:\n",
      "          Predict Counter({'vgood': 4})\n",
      "      --> Branch vhigh:\n",
      "        Predict Counter({'acc': 5})\n",
      "    --> Branch vhigh:\n",
      "      Price of maintenance\n",
      "      --> Branch high:\n",
      "        Predict Counter({'unacc': 12})\n",
      "      --> Branch low:\n",
      "        Predict Counter({'acc': 10})\n",
      "      --> Branch med:\n",
      "        Predict Counter({'acc': 7})\n",
      "      --> Branch vhigh:\n",
      "        Predict Counter({'unacc': 5})\n",
      "  --> Branch low:\n",
      "    Predict Counter({'unacc': 110})\n",
      "  --> Branch med:\n",
      "    Size of luggage boot\n",
      "    --> Branch big:\n",
      "      Price of maintenance\n",
      "      --> Branch high:\n",
      "        Buying Price\n",
      "        --> Branch high:\n",
      "          Predict Counter({'acc': 1})\n",
      "        --> Branch low:\n",
      "          Predict Counter({'acc': 5})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'acc': 2})\n",
      "        --> Branch vhigh:\n",
      "          Predict Counter({'unacc': 3})\n",
      "      --> Branch low:\n",
      "        Predict Counter({'good': 2})\n",
      "      --> Branch med:\n",
      "        Buying Price\n",
      "        --> Branch high:\n",
      "          Predict Counter({'acc': 5})\n",
      "        --> Branch low:\n",
      "          Predict Counter({'good': 2})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'acc': 3})\n",
      "        --> Branch vhigh:\n",
      "          Predict Counter({'acc': 3})\n",
      "      --> Branch vhigh:\n",
      "        Buying Price\n",
      "        --> Branch high:\n",
      "          Predict Counter({'unacc': 1})\n",
      "        --> Branch low:\n",
      "          Predict Counter({'acc': 5})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'acc': 1})\n",
      "    --> Branch med:\n",
      "      Price of maintenance\n",
      "      --> Branch high:\n",
      "        Buying Price\n",
      "        --> Branch high:\n",
      "          Predict Counter({'acc': 2})\n",
      "        --> Branch low:\n",
      "          Predict Counter({'acc': 3})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'unacc': 1})\n",
      "      --> Branch low:\n",
      "        Buying Price\n",
      "        --> Branch high:\n",
      "          Predict Counter({'unacc': 1})\n",
      "        --> Branch low:\n",
      "          Predict Counter({'good': 1})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'good': 1})\n",
      "        --> Branch vhigh:\n",
      "          Predict Counter({'acc': 2})\n",
      "      --> Branch med:\n",
      "        Number of doors\n",
      "        --> Branch 2:\n",
      "          Predict Counter({'acc': 2})\n",
      "        --> Branch 4:\n",
      "          Predict Counter({'good': 1})\n",
      "        --> Branch 5more:\n",
      "          Predict Counter({'acc': 1})\n",
      "      --> Branch vhigh:\n",
      "        Buying Price\n",
      "        --> Branch high:\n",
      "          Predict Counter({'unacc': 1})\n",
      "        --> Branch low:\n",
      "          Predict Counter({'unacc': 2})\n",
      "        --> Branch med:\n",
      "          Number of doors\n",
      "          --> Branch 3:\n",
      "            Predict Counter({'unacc': 1})\n",
      "          --> Branch 5more:\n",
      "            Predict Counter({'acc': 1})\n",
      "        --> Branch vhigh:\n",
      "          Predict Counter({'unacc': 1})\n",
      "    --> Branch small:\n",
      "      Buying Price\n",
      "      --> Branch high:\n",
      "        Predict Counter({'unacc': 18})\n",
      "      --> Branch low:\n",
      "        Price of maintenance\n",
      "        --> Branch high:\n",
      "          Predict Counter({'acc': 4})\n",
      "        --> Branch low:\n",
      "          Predict Counter({'acc': 2})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'acc': 3})\n",
      "        --> Branch vhigh:\n",
      "          Predict Counter({'unacc': 3})\n",
      "      --> Branch med:\n",
      "        Price of maintenance\n",
      "        --> Branch high:\n",
      "          Predict Counter({'unacc': 1})\n",
      "        --> Branch low:\n",
      "          Predict Counter({'acc': 3})\n",
      "        --> Branch vhigh:\n",
      "          Predict Counter({'unacc': 4})\n",
      "      --> Branch vhigh:\n",
      "        Predict Counter({'unacc': 13})\n",
      "--> Branch more:\n",
      "  Estimated Saftey\n",
      "  --> Branch high:\n",
      "    Buying Price\n",
      "    --> Branch high:\n",
      "      Price of maintenance\n",
      "      --> Branch high:\n",
      "        Predict Counter({'acc': 5})\n",
      "      --> Branch low:\n",
      "        Predict Counter({'acc': 6})\n",
      "      --> Branch med:\n",
      "        Number of doors\n",
      "        --> Branch 2:\n",
      "          Size of luggage boot\n",
      "          --> Branch big:\n",
      "            Predict Counter({'acc': 1})\n",
      "          --> Branch small:\n",
      "            Predict Counter({'unacc': 3})\n",
      "        --> Branch 5more:\n",
      "          Predict Counter({'acc': 2})\n",
      "      --> Branch vhigh:\n",
      "        Predict Counter({'unacc': 11})\n",
      "    --> Branch low:\n",
      "      Price of maintenance\n",
      "      --> Branch high:\n",
      "        Size of luggage boot\n",
      "        --> Branch big:\n",
      "          Predict Counter({'vgood': 2})\n",
      "        --> Branch small:\n",
      "          Predict Counter({'acc': 1})\n",
      "      --> Branch low:\n",
      "        Predict Counter({'vgood': 5})\n",
      "      --> Branch med:\n",
      "        Number of doors\n",
      "        --> Branch 2:\n",
      "          Size of luggage boot\n",
      "          --> Branch med:\n",
      "            Predict Counter({'good': 2})\n",
      "          --> Branch small:\n",
      "            Predict Counter({'unacc': 4})\n",
      "        --> Branch 3:\n",
      "          Predict Counter({'vgood': 1})\n",
      "        --> Branch 4:\n",
      "          Predict Counter({'vgood': 3})\n",
      "        --> Branch 5more:\n",
      "          Predict Counter({'vgood': 2})\n",
      "      --> Branch vhigh:\n",
      "        Number of doors\n",
      "        --> Branch 2:\n",
      "          Size of luggage boot\n",
      "          --> Branch big:\n",
      "            Predict Counter({'acc': 1})\n",
      "          --> Branch small:\n",
      "            Predict Counter({'unacc': 1})\n",
      "        --> Branch 3:\n",
      "          Predict Counter({'acc': 3})\n",
      "        --> Branch 4:\n",
      "          Predict Counter({'acc': 3})\n",
      "        --> Branch 5more:\n",
      "          Predict Counter({'acc': 5})\n",
      "    --> Branch med:\n",
      "      Price of maintenance\n",
      "      --> Branch high:\n",
      "        Predict Counter({'acc': 6})\n",
      "      --> Branch low:\n",
      "        Number of doors\n",
      "        --> Branch 2:\n",
      "          Predict Counter({'unacc': 1})\n",
      "        --> Branch 3:\n",
      "          Predict Counter({'good': 1})\n",
      "        --> Branch 5more:\n",
      "          Predict Counter({'good': 1})\n",
      "      --> Branch med:\n",
      "        Number of doors\n",
      "        --> Branch 2:\n",
      "          Predict Counter({'acc': 2})\n",
      "        --> Branch 4:\n",
      "          Predict Counter({'vgood': 1})\n",
      "      --> Branch vhigh:\n",
      "        Predict Counter({'acc': 7})\n",
      "    --> Branch vhigh:\n",
      "      Price of maintenance\n",
      "      --> Branch high:\n",
      "        Predict Counter({'unacc': 8})\n",
      "      --> Branch low:\n",
      "        Predict Counter({'acc': 10})\n",
      "      --> Branch med:\n",
      "        Predict Counter({'acc': 7})\n",
      "      --> Branch vhigh:\n",
      "        Predict Counter({'unacc': 12})\n",
      "  --> Branch low:\n",
      "    Predict Counter({'unacc': 105})\n",
      "  --> Branch med:\n",
      "    Price of maintenance\n",
      "    --> Branch high:\n",
      "      Buying Price\n",
      "      --> Branch high:\n",
      "        Number of doors\n",
      "        --> Branch 2:\n",
      "          Size of luggage boot\n",
      "          --> Branch big:\n",
      "            Predict Counter({'acc': 1})\n",
      "          --> Branch med:\n",
      "            Predict Counter({'unacc': 1})\n",
      "        --> Branch 3:\n",
      "          Predict Counter({'acc': 4})\n",
      "        --> Branch 4:\n",
      "          Predict Counter({'acc': 2})\n",
      "        --> Branch 5more:\n",
      "          Predict Counter({'acc': 3})\n",
      "      --> Branch low:\n",
      "        Predict Counter({'acc': 4})\n",
      "      --> Branch med:\n",
      "        Size of luggage boot\n",
      "        --> Branch big:\n",
      "          Predict Counter({'acc': 1})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'acc': 4})\n",
      "        --> Branch small:\n",
      "          Predict Counter({'unacc': 2})\n",
      "      --> Branch vhigh:\n",
      "        Predict Counter({'unacc': 3})\n",
      "    --> Branch low:\n",
      "      Buying Price\n",
      "      --> Branch high:\n",
      "        Predict Counter({'acc': 4})\n",
      "      --> Branch low:\n",
      "        Size of luggage boot\n",
      "        --> Branch big:\n",
      "          Predict Counter({'good': 7})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'good': 8})\n",
      "        --> Branch small:\n",
      "          Number of doors\n",
      "          --> Branch 2:\n",
      "            Predict Counter({'unacc': 1})\n",
      "          --> Branch 3:\n",
      "            Predict Counter({'acc': 1})\n",
      "      --> Branch med:\n",
      "        Number of doors\n",
      "        --> Branch 2:\n",
      "          Predict Counter({'unacc': 1})\n",
      "        --> Branch 4:\n",
      "          Size of luggage boot\n",
      "          --> Branch med:\n",
      "            Predict Counter({'good': 1})\n",
      "          --> Branch small:\n",
      "            Predict Counter({'acc': 2})\n",
      "        --> Branch 5more:\n",
      "          Predict Counter({'good': 2})\n",
      "      --> Branch vhigh:\n",
      "        Predict Counter({'acc': 6})\n",
      "    --> Branch med:\n",
      "      Number of doors\n",
      "      --> Branch 2:\n",
      "        Size of luggage boot\n",
      "        --> Branch big:\n",
      "          Predict Counter({'acc': 2})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'unacc': 1})\n",
      "        --> Branch small:\n",
      "          Predict Counter({'unacc': 5})\n",
      "      --> Branch 4:\n",
      "        Buying Price\n",
      "        --> Branch high:\n",
      "          Size of luggage boot\n",
      "          --> Branch med:\n",
      "            Predict Counter({'acc': 1})\n",
      "          --> Branch small:\n",
      "            Predict Counter({'unacc': 1})\n",
      "        --> Branch low:\n",
      "          Predict Counter({'acc': 1})\n",
      "        --> Branch vhigh:\n",
      "          Predict Counter({'acc': 1})\n",
      "      --> Branch 5more:\n",
      "        Buying Price\n",
      "        --> Branch low:\n",
      "          Predict Counter({'good': 1})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'acc': 2})\n",
      "        --> Branch vhigh:\n",
      "          Predict Counter({'acc': 1})\n",
      "    --> Branch vhigh:\n",
      "      Size of luggage boot\n",
      "      --> Branch big:\n",
      "        Buying Price\n",
      "        --> Branch high:\n",
      "          Predict Counter({'unacc': 2})\n",
      "        --> Branch low:\n",
      "          Predict Counter({'acc': 2})\n",
      "        --> Branch med:\n",
      "          Predict Counter({'acc': 2})\n",
      "        --> Branch vhigh:\n",
      "          Predict Counter({'unacc': 1})\n",
      "      --> Branch med:\n",
      "        Number of doors\n",
      "        --> Branch 2:\n",
      "          Predict Counter({'unacc': 1})\n",
      "        --> Branch 3:\n",
      "          Predict Counter({'acc': 1})\n",
      "        --> Branch 5more:\n",
      "          Predict Counter({'unacc': 1})\n",
      "      --> Branch small:\n",
      "        Predict Counter({'unacc': 10})\n"
     ]
    }
   ],
   "source": [
    "#from tree import build_tree, print_tree, car_data, car_labels\n",
    "import random\n",
    "random.seed(4)\n",
    "\n",
    "tree = build_tree(car_data,car_labels)\n",
    "#print_tree(tree)\n",
    "\n",
    "\n",
    "# indices = []\n",
    "# for i in range(1000):\n",
    "#   indices.append(random.randint(0,999))\n",
    "\n",
    "# data_subset =[]\n",
    "# labels_subset = []\n",
    "\n",
    "# for index in indices:\n",
    "#   data_subset.append(car_data[index])\n",
    "\n",
    "indices = [random.randint(0,999) for i in range(1000)]\n",
    "\n",
    "data_subset = [car_data[index] for index in indices]\n",
    "\n",
    "labels_subset = [car_labels[index] for index in indices]\n",
    "\n",
    "subset_tree = build_tree(data_subset, labels_subset)\n",
    "\n",
    "print_tree(subset_tree)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Features\n",
    "\n",
    "We’re now making trees based on different random subsets of our initial dataset. But we can continue to add variety to the ways our trees are created by changing the features that we use.\n",
    "\n",
    "Recall that for our car data set, the original features were the following:\n",
    "\n",
    "The price of the car\n",
    "The cost of maintenance\n",
    "The number of doors\n",
    "The number of people the car can hold\n",
    "The size of the trunk\n",
    "The safety rating\n",
    "Right now when we create a decision tree, we look at every one of those features and choose to split the data based on the feature that produces the most information gain. We could change how the tree is created by only allowing a subset of those features to be considered at each split.\n",
    "\n",
    "For example, when finding which feature to split the data on the first time, we might randomly choose to only consider the price of the car, the number of doors, and the safety rating.\n",
    "\n",
    "After splitting the data on the best feature from that subset, we’ll likely want to split again. For this next split, we’ll randomly select three features again to consider. This time those features might be the cost of maintenance, the number of doors, and the size of the trunk. We’ll continue this process until the tree is complete.\n",
    "\n",
    "One question to consider is how to choose the number of features to randomly select. Why did we choose 3 in this example? A good rule of thumb is to randomly select the square root of the total number of features. Our car dataset doesn’t have a lot of features, so in this example, it’s difficult to follow this rule. But if we had a dataset with 25 features, we’d want to randomly select 5 features to consider at every split point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "1.\n",
    "We’ve given you access to the code that finds the best feature to split on. Right now, it considers all possible features. We’re going to want to change that!\n",
    "\n",
    "For now, let’s see what the best feature to split the dataset is. At the bottom of your code, call find_best_split() using data_subset and labels_subset as parameters and print the results.\n",
    "\n",
    "This function returns the information gain and the index of the best feature. What was the index?\n",
    "\n",
    "That index corresponds to the features of our car. For example, if the best feature index to split on was 0, that means we’re splitting on the price of the car.\n",
    "\n",
    "2.\n",
    "We now want to modify our find_best_split() function to only consider a subset of the features. We want to pick 3 features without replacement.\n",
    "\n",
    "The random.choice() function found in Python’s numpy module can help us do this. random.choice() returns a list of values between 0 and the first parameter. The size of the list is determined by the second parameter. And we can choose without replacement by setting replace = False.\n",
    "\n",
    "For example, the following code would choose ten unique numbers between 0 and 100 (exclusive) and put them in a list.\n",
    "\n",
    "lst = np.random.choice(100, 10, replace = False)\n",
    "Inside find_best_split(), create a list named features that contains 3 numbers between 0 and len(dataset[0]).\n",
    "\n",
    "Instead of looping through feature in range(len(dataset[0])), loop through feature in features.\n",
    "\n",
    "Now that we’ve implemented feature bagging, what is the best index to use as the split index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.010225712539814483, 4)\n"
     ]
    }
   ],
   "source": [
    "# from tree import car_data, car_labels, split, information_gain\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "random.seed(4)\n",
    "\n",
    "def find_best_split(dataset, labels):\n",
    "    best_gain = 0\n",
    "    best_feature = 0\n",
    "    #Create features here\n",
    "    features = np.random.choice(len(dataset[0]), 3, replace=False)\n",
    "    for feature in features:\n",
    "        data_subsets, label_subsets = split(dataset, labels, feature)\n",
    "        gain = information_gain(labels, label_subsets)\n",
    "        if gain > best_gain:\n",
    "            best_gain, best_feature = gain, feature\n",
    "    return best_gain, best_feature\n",
    "  \n",
    "indices = [random.randint(0, 999) for i in range(1000)]\n",
    "\n",
    "data_subset = [car_data[index] for index in indices]\n",
    "labels_subset = [car_labels[index] for index in indices]\n",
    "print(find_best_split(data_subset, labels_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify\n",
    "\n",
    "Now that we can make different decision trees, it’s time to plant a whole forest! Let’s say we make different 8 trees using bagging and feature bagging. We can now take a new unlabeled point, give that point to each tree in the forest, and count the number of times different labels are predicted.\n",
    "\n",
    "The trees give us their votes and the label that is predicted most often will be our final classification! For example, if we gave our random forest of 8 trees a new data point, we might get the following results:\n",
    "\n",
    "[\"vgood\", \"vgood\", \"good\", \"vgood\", \"acc\", \"vgood\", \"good\", \"vgood\"]\n",
    "\n",
    "Since the most commonly predicted classification was \"vgood\", this would be the random forest’s final classification.\n",
    "\n",
    "Let’s write some code that can classify an unlabeled point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "def split(dataset, labels, column):\n",
    "    data_subsets = []\n",
    "    label_subsets = []\n",
    "    counts = list(set([data[column] for data in dataset]))\n",
    "    counts.sort()\n",
    "    for k in counts:\n",
    "        new_data_subset = []\n",
    "        new_label_subset = []\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset[i][column] == k:\n",
    "                new_data_subset.append(dataset[i])\n",
    "                new_label_subset.append(labels[i])\n",
    "        data_subsets.append(new_data_subset)\n",
    "        label_subsets.append(new_label_subset)\n",
    "    return data_subsets, label_subsets\n",
    "\n",
    "def gini(dataset):\n",
    "  impurity = 1\n",
    "  label_counts = Counter(dataset)\n",
    "  for label in label_counts:\n",
    "    prob_of_label = label_counts[label] / len(dataset)\n",
    "    impurity -= prob_of_label ** 2\n",
    "  return impurity\n",
    "\n",
    "def information_gain(starting_labels, split_labels):\n",
    "  info_gain = gini(starting_labels)\n",
    "  for subset in split_labels:\n",
    "    info_gain -= gini(subset) * len(subset)/len(starting_labels)\n",
    "  return info_gain\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, labels, value):\n",
    "        self.labels = Counter(labels)\n",
    "        self.value = value\n",
    "\n",
    "class Internal_Node:\n",
    "    def __init__(self,\n",
    "                 feature,\n",
    "                 branches,\n",
    "                 value):\n",
    "        self.feature = feature\n",
    "        self.branches = branches\n",
    "        self.value = value\n",
    "\n",
    "def find_best_split_subset(dataset, labels, num_features):\n",
    "    features = np.random.choice(6, 3, replace=False)\n",
    "    best_gain = 0\n",
    "    best_feature = 0\n",
    "    for feature in features:\n",
    "        data_subsets, label_subsets = split(dataset, labels, feature)\n",
    "        gain = information_gain(labels, label_subsets)\n",
    "        if gain > best_gain:\n",
    "            best_gain, best_feature = gain, feature\n",
    "    return best_feature, best_gain\n",
    "\n",
    "def find_best_split(dataset, labels):\n",
    "    best_gain = 0\n",
    "    best_feature = 0\n",
    "    for feature in range(len(dataset[0])):\n",
    "        data_subsets, label_subsets = split(dataset, labels, feature)\n",
    "        gain = information_gain(labels, label_subsets)\n",
    "        if gain > best_gain:\n",
    "            best_gain, best_feature = gain, feature\n",
    "    return best_feature, best_gain\n",
    "\n",
    "def build_tree(data, labels, value = \"\"):\n",
    "  best_feature, best_gain = find_best_split(data, labels)\n",
    "  if best_gain < 0.00000001:\n",
    "    return Leaf(Counter(labels), value)\n",
    "  data_subsets, label_subsets = split(data, labels, best_feature)\n",
    "  branches = []\n",
    "  for i in range(len(data_subsets)):\n",
    "    branch = build_tree(data_subsets[i], label_subsets[i], data_subsets[i][0][best_feature])\n",
    "    branches.append(branch)\n",
    "  return Internal_Node(best_feature, branches, value)\n",
    "\n",
    "def build_tree_forest(data,labels, n_features, value=\"\"):\n",
    "    best_feature, best_gain = find_best_split_subset(data, labels, n_features)\n",
    "    if best_gain < 0.00000001:\n",
    "      return Leaf(Counter(labels), value)\n",
    "    data_subsets, label_subsets = split(data, labels, best_feature)\n",
    "    branches = []\n",
    "    for i in range(len(data_subsets)):\n",
    "      branch = build_tree_forest(data_subsets[i], label_subsets[i], n_features, data_subsets[i][0][best_feature])\n",
    "      branches.append(branch)\n",
    "    return Internal_Node(best_feature, branches, value)\n",
    "\n",
    "\n",
    "def print_tree(node, spacing=\"\"):\n",
    "    \"\"\"World's most elegant tree printing function.\"\"\"\n",
    "    question_dict = {0: \"Buying Price\", 1:\"Price of maintenance\", 2:\"Number of doors\", 3:\"Person Capacity\", 4:\"Size of luggage boot\", 5:\"Estimated Saftey\"}\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + str(node.labels))\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + \"Splitting on \" + question_dict[node.feature])\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    for i in range(len(node.branches)):\n",
    "        print (spacing + '--> Branch ' + node.branches[i].value+':')\n",
    "        print_tree(node.branches[i], spacing + \"  \")\n",
    "\n",
    "\n",
    "def make_cars():\n",
    "    f = open(\"car.csv\", \"r\")\n",
    "    cars = []\n",
    "    for line in f:\n",
    "        cars.append(line.rstrip().split(\",\"))\n",
    "    return cars\n",
    "\n",
    "\n",
    "\n",
    "def change_data(data):\n",
    "    dicts = [{'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},\n",
    "    {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},\n",
    "    {'2' : 1.0, '3' : 2.0, '4' : 3.0, '5more' : 4.0},\n",
    "    {'2' : 1.0, '4' : 2.0, 'more' : 3.0},\n",
    "    {'small' : 1.0, 'med' : 2.0, 'big' : 3.0},\n",
    "    {'low' : 1.0, 'med' : 2.0, 'high' : 3.0}]\n",
    "\n",
    "    for row in data:\n",
    "        for i in range(len(dicts)):\n",
    "            row[i] = dicts[i][row[i]]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def classify(datapoint, tree):\n",
    "  if isinstance(tree, Leaf):\n",
    "    return max(tree.labels.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "  value = datapoint[tree.feature]\n",
    "  for branch in tree.branches:\n",
    "    if branch.value == value:\n",
    "      return classify(datapoint, branch)\n",
    "  #return classify(datapoint, tree.branches[random.randint(0, len(tree.branches)-1)])\n",
    "\n",
    "\n",
    "\n",
    "cars = make_cars()\n",
    "random.shuffle(cars)\n",
    "car_data = [x[:-1] for x in cars]\n",
    "car_labels = [x[-1] for x in cars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qurstion\n",
    "1.\n",
    "At the top of your code, we’ve included a new unlabeled car named unlabeled_point that we want to classify. We’ve also created a tree named subset_tree that was created using bagging and feature bagging.\n",
    "\n",
    "Let’s see how that tree classifies this point. Print the results of classify() using unlabeled_point and subset_tree as parameters.\n",
    "\n",
    "\n",
    "Stuck? Get a hint\n",
    "2.\n",
    "That’s the prediction using one tree. Let’s make 20 trees and record the prediction of each one!\n",
    "\n",
    "Take all of your code between creating indices and the print statement you just wrote and put it in a for loop that happens 20 times.\n",
    "\n",
    "Above your for loop, create a variable named predictions and set it equal to an empty list. Inside your for loop, instead of printing the prediction, use .append() to add it to predictions.\n",
    "\n",
    "Finally after your for loop, print predictions.\n",
    "\n",
    "\n",
    "Stuck? Get a hint\n",
    "3.\n",
    "We now have a list of 20 predictions — let’s find the most common one! You can find the most common element in a list by using this line of code:\n",
    "\n",
    "max(predictions, key=predictions.count)\n",
    "Outside of your for loop, store the most common element in a variable named final_prediction and print that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acc', 'unacc', 'acc', 'unacc', None, 'acc', 'acc', 'unacc', 'unacc', None, 'acc', 'unacc', 'acc', 'acc', 'acc', 'acc', 'acc', 'unacc', None, 'acc']\n",
      "\n",
      "acc\n"
     ]
    }
   ],
   "source": [
    "#from tree import build_tree, print_tree, car_data, car_labels, classify\n",
    "import random\n",
    "random.seed(4)\n",
    "\n",
    "# The features are the price of the car, the cost of maintenance, the number of doors, the number of people the car can hold, the size of the trunk, and the safety rating\n",
    "unlabeled_point = ['high', 'vhigh', '3', 'more', 'med', 'med']\n",
    "\n",
    "predictions = []\n",
    "for i in range(20):\n",
    "  indices = [random.randint(0, 999) for i in range(1000)]\n",
    "  data_subset = [car_data[index] for index in indices]\n",
    "  labels_subset = [car_labels[index] for index in indices]\n",
    "  subset_tree = build_tree(data_subset, labels_subset)\n",
    "\n",
    "\n",
    "  predictions.append(classify(unlabeled_point,subset_tree))\n",
    "\n",
    "\n",
    "print(predictions)\n",
    "print('')\n",
    "final_prediction = max(predictions, key=predictions.count)\n",
    "print(final_prediction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set\n",
    "\n",
    "We’re now able to create a random forest, but how accurate is it compared to a single decision tree? To answer this question we’ve split our data into a training set and test set. By building our models using the training set and testing on every data point in the test set, we can calculate the accuracy of both a single decision tree and a random forest.\n",
    "\n",
    "We’ve given you code that calculates the accuracy of a single tree. This tree was made without using any of the bagging techniques we just learned. We created the tree by using every row from the training set once and considered every feature when splitting the data rather than a random subset.\n",
    "\n",
    "Let’s also calculate the accuracy of a random forest and see how it compares!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "Begin by taking a look at the code we’ve given you. We’ve created a single tree using the training data, looped through every point in the test set, counted the number of points the tree classified correctly and reported the percentage of correctly classified points — this percentage is known as the accuracy of the model.\n",
    "\n",
    "Run the code to see the accuracy of the single decision tree.\n",
    "\n",
    "2.\n",
    "Right below where tree is created, create a random forest named forest using our make_random_forest() function.\n",
    "\n",
    "This function takes three parameters — the number of trees in the forest, the training data, and the training labels. It returns a list of trees.\n",
    "\n",
    "Create a random forest with 40 trees using training_data and training_labels.\n",
    "\n",
    "You should also create a variable named forest_correct and start it at 0. This is the variable that will keep track of how many points in the test set the random forest correctly classifies.\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "For every data point in the test set, we want every tree to classify the data point, find the most common classification, and compare that prediction to the true label of the data point. This is very similar to what you did in the previous exercise.\n",
    "\n",
    "To begin, at the end of the for loop outside the if statement, create an empty list named predictions. Next, loop through every forest_tree in forest. Call classify() using testing_data[i] and forest_tree as parameters and append the result to predictions.\n",
    "\n",
    "\n",
    "\n",
    "4.\n",
    "After we loop through every tree in the forest, we now want to find the most common prediction and compare it to the true label. The true label can be found using testing_labels[i]. If they’re equal, we’ve correctly classified a point and should add 1 to forest_correct.\n",
    "\n",
    "An easy way of finding the most common prediction is by using this line of code:\n",
    "\n",
    "forest_prediction = max(predictions,key=predictions.count)\n",
    "\n",
    "\n",
    "5.\n",
    "Finally, after looping through all of the points in the test set, we want to print out the accuracy of our random forest. Divide forest_correct by the number of items in the test set and print the result.\n",
    "\n",
    "How did the random forest do compared to the single decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "def split(dataset, labels, column):\n",
    "    data_subsets = []\n",
    "    label_subsets = []\n",
    "    counts = list(set([data[column] for data in dataset]))\n",
    "    counts.sort()\n",
    "    for k in counts:\n",
    "        new_data_subset = []\n",
    "        new_label_subset = []\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset[i][column] == k:\n",
    "                new_data_subset.append(dataset[i])\n",
    "                new_label_subset.append(labels[i])\n",
    "        data_subsets.append(new_data_subset)\n",
    "        label_subsets.append(new_label_subset)\n",
    "    return data_subsets, label_subsets\n",
    "\n",
    "def gini(dataset):\n",
    "  impurity = 1\n",
    "  label_counts = Counter(dataset)\n",
    "  for label in label_counts:\n",
    "    prob_of_label = label_counts[label] / len(dataset)\n",
    "    impurity -= prob_of_label ** 2\n",
    "  return impurity\n",
    "\n",
    "def information_gain(starting_labels, split_labels):\n",
    "  info_gain = gini(starting_labels)\n",
    "  for subset in split_labels:\n",
    "    info_gain -= gini(subset) * len(subset)/len(starting_labels)\n",
    "  return info_gain\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, labels, value):\n",
    "        self.labels = Counter(labels)\n",
    "        self.value = value\n",
    "\n",
    "class Internal_Node:\n",
    "    def __init__(self,\n",
    "                 feature,\n",
    "                 branches,\n",
    "                 value):\n",
    "        self.feature = feature\n",
    "        self.branches = branches\n",
    "        self.value = value\n",
    "\n",
    "def find_best_split_subset(dataset, labels, num_features):\n",
    "    features = np.random.choice(6, 3, replace=False)\n",
    "    best_gain = 0\n",
    "    best_feature = 0\n",
    "    for feature in features:\n",
    "        data_subsets, label_subsets = split(dataset, labels, feature)\n",
    "        gain = information_gain(labels, label_subsets)\n",
    "        if gain > best_gain:\n",
    "            best_gain, best_feature = gain, feature\n",
    "    return best_feature, best_gain\n",
    "\n",
    "def find_best_split(dataset, labels):\n",
    "    best_gain = 0\n",
    "    best_feature = 0\n",
    "    for feature in range(len(dataset[0])):\n",
    "        data_subsets, label_subsets = split(dataset, labels, feature)\n",
    "        gain = information_gain(labels, label_subsets)\n",
    "        if gain > best_gain:\n",
    "            best_gain, best_feature = gain, feature\n",
    "    return best_feature, best_gain\n",
    "\n",
    "def make_single_tree(data, labels, value = \"\"):\n",
    "  best_feature, best_gain = find_best_split(data, labels)\n",
    "  if best_gain < 0.00000001:\n",
    "    return Leaf(Counter(labels), value)\n",
    "  data_subsets, label_subsets = split(data, labels, best_feature)\n",
    "  branches = []\n",
    "  for i in range(len(data_subsets)):\n",
    "    branch = make_single_tree(data_subsets[i], label_subsets[i], data_subsets[i][0][best_feature])\n",
    "    branches.append(branch)\n",
    "  return Internal_Node(best_feature, branches, value)\n",
    "\n",
    "def build_tree_forest(data,labels, n_features, value=\"\"):\n",
    "    best_feature, best_gain = find_best_split_subset(data, labels, n_features)\n",
    "    if best_gain < 0.00000001:\n",
    "      return Leaf(Counter(labels), value)\n",
    "    data_subsets, label_subsets = split(data, labels, best_feature)\n",
    "    branches = []\n",
    "    for i in range(len(data_subsets)):\n",
    "      branch = build_tree_forest(data_subsets[i], label_subsets[i], n_features, data_subsets[i][0][best_feature])\n",
    "      branches.append(branch)\n",
    "    return Internal_Node(best_feature, branches, value)\n",
    "\n",
    "def print_tree(node, spacing=\"\"):\n",
    "    \"\"\"World's most elegant tree printing function.\"\"\"\n",
    "    question_dict = {0: \"Buying Price\", 1:\"Price of maintenance\", 2:\"Number of doors\", 3:\"Person Capacity\", 4:\"Size of luggage boot\", 5:\"Estimated Saftey\"}\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + str(node.labels))\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + \"Splitting on \" + question_dict[node.feature])\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    for i in range(len(node.branches)):\n",
    "        print (spacing + '--> Branch ' + node.branches[i].value+':')\n",
    "        print_tree(node.branches[i], spacing + \"  \")\n",
    "\n",
    "def make_cars():\n",
    "    f = open(\"car.csv\", \"r\")\n",
    "    cars = []\n",
    "    for line in f:\n",
    "        cars.append(line.rstrip().split(\",\"))\n",
    "    return cars\n",
    "\n",
    "def change_data(data):\n",
    "    dicts = [{'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},\n",
    "    {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},\n",
    "    {'2' : 1.0, '3' : 2.0, '4' : 3.0, '5more' : 4.0},\n",
    "    {'2' : 1.0, '4' : 2.0, 'more' : 3.0},\n",
    "    {'small' : 1.0, 'med' : 2.0, 'big' : 3.0},\n",
    "    {'low' : 1.0, 'med' : 2.0, 'high' : 3.0}]\n",
    "\n",
    "    for row in data:\n",
    "        for i in range(len(dicts)):\n",
    "            row[i] = dicts[i][row[i]]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def classify(datapoint, tree):\n",
    "  if isinstance(tree, Leaf):\n",
    "    items = list(tree.labels.items()) \n",
    "    items.sort()\n",
    "    return max(items, key=operator.itemgetter(1))[0]\n",
    "\n",
    "  value = datapoint[tree.feature]\n",
    "  for branch in tree.branches:\n",
    "    if branch.value == value:\n",
    "      return classify(datapoint, branch)\n",
    "  #return classify(datapoint, tree.branches[random.randint(0, len(tree.branches)-1)])\n",
    "\n",
    "\n",
    "\n",
    "cars = make_cars()\n",
    "random.shuffle(cars)\n",
    "car_data = [x[:-1] for x in cars]\n",
    "car_labels = [x[-1] for x in cars]\n",
    "# car_data = car_data[:500]\n",
    "# car_labels = car_labels[:500]\n",
    "\n",
    "\n",
    "training_data = car_data[:int(len(car_data)*0.8)]\n",
    "training_labels = car_labels[:int(len(car_data)*0.8)]\n",
    "\n",
    "\n",
    "testing_data = car_data[int(len(car_data)*0.8):]\n",
    "testing_labels = car_labels[int(len(car_data)*0.8):]\n",
    "\n",
    "def make_random_forest(n, training_data, training_labels):\n",
    "    trees = []\n",
    "    for i in range(n):\n",
    "        indices = [random.randint(0, len(training_data)-1) for x in range(len(training_data))]\n",
    "        training_data_subset = [training_data[index] for index in indices]\n",
    "        training_labels_subset = [training_labels[index] for index in indices]\n",
    "\n",
    "        tree = build_tree_forest(training_data_subset, training_labels_subset, 2)\n",
    "        trees.append(tree)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8815028901734104\n",
      "0.9219653179190751\n"
     ]
    }
   ],
   "source": [
    "#from tree import training_data, training_labels, testing_data, testing_labels, make_random_forest, make_single_tree, classify\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "from collections import Counter\n",
    "\n",
    "tree = make_single_tree(training_data, training_labels)\n",
    "single_tree_correct = 0\n",
    "\n",
    "forest = make_random_forest(40, training_data, training_labels)\n",
    "forest_correct = 0\n",
    "\n",
    "for i in range(len(testing_data)):\n",
    "  prediction = classify(testing_data[i], tree)\n",
    "  if prediction == testing_labels[i]:\n",
    "    single_tree_correct += 1\n",
    "  predictions = []\n",
    "  for forest_tree in forest:\n",
    "    predictions.append(classify(testing_data[i], forest_tree))\n",
    "  forest_prediction = max(predictions,key=predictions.count)\n",
    "  if forest_prediction == testing_labels[i]:\n",
    "    forest_correct += 1\n",
    "    \n",
    "print(single_tree_correct/len(testing_data))\n",
    "print(forest_correct/len(testing_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest in Scikit-learn\n",
    "You now have the ability to make a random forest using your own decision trees. However, scikit-learn has a RandomForestClassifier class that will do all of this work for you! RandomForestClassifier is in the sklearn.ensemble module.\n",
    "\n",
    "RandomForestClassifier works almost identically to DecisionTreeClassifier — the .fit(), .predict(), and .score() methods work in the exact same way.\n",
    "\n",
    "When creating a RandomForestClassifier, you can choose how many trees to include in the random forest by using the n_estimators parameter like this:\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators = 100)\n",
    "We now have a very powerful machine learning model that is fairly resistant to overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "def make_cars():\n",
    "    f = open(\"car.csv\", \"r\")\n",
    "    cars = []\n",
    "    for line in f:\n",
    "        cars.append(line.rstrip().split(\",\"))\n",
    "    return cars\n",
    "  \n",
    "def change_data(data):\n",
    "    dicts = [{'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},\n",
    "    {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0},\n",
    "    {'2' : 1.0, '3' : 2.0, '4' : 3.0, '5more' : 4.0},\n",
    "    {'2' : 1.0, '4' : 2.0, 'more' : 3.0},\n",
    "    {'small' : 1.0, 'med' : 2.0, 'big' : 3.0},\n",
    "    {'low' : 1.0, 'med' : 2.0, 'high' : 3.0}]\n",
    "\n",
    "    for row in data:\n",
    "        for i in range(len(dicts)):\n",
    "            row[i] = dicts[i][row[i]]\n",
    "\n",
    "    return data\n",
    "  \n",
    "cars = change_data(make_cars())\n",
    "random.shuffle(cars)\n",
    "car_data = [x[:-1] for x in cars]\n",
    "car_labels = [x[-1] for x in cars]\n",
    "\n",
    "training_points = car_data[:int(len(car_data)*0.9)]\n",
    "training_labels = car_labels[:int(len(car_labels)*0.9)]\n",
    "\n",
    "testing_points = car_data[int(len(car_data)*0.9):]\n",
    "testing_labels = car_labels[int(len(car_labels)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9826589595375722\n"
     ]
    }
   ],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "#from cars import training_points, training_labels, testing_points, testing_labels\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators = 2000, random_state=0)\n",
    "\n",
    "classifier.fit(training_points,training_labels)\n",
    "\n",
    "score = classifier.score(testing_points,testing_labels)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "Nice work! Here are some of the major takeaways about random forests:\n",
    "\n",
    "A random forest is an ensemble machine learning model. It makes a classification by aggregating the classifications of many decision trees.\n",
    "Random forests are used to avoid overfitting. By aggregating the classification of multiple trees, having overfitted trees in a random forest is less impactful.\n",
    "Every decision tree in a random forest is created by using a different subset of data points from the training set. Those data points are chosen at random with replacement, which means a single data point can be chosen more than once. This process is known as bagging.\n",
    "When creating a tree in a random forest, a randomly selected subset of features are considered as candidates for the best splitting feature. If your dataset has n features, it is common practice to randomly select the square root of n features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
