{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION\n",
    "## Introduction\n",
    "When an email lands in your inbox, how does your email service know whether it’s a real email or spam? This evaluation is made billions of times per day, and one way it can be done is with Logistic Regression. Logistic Regression is a supervised machine learning algorithm that uses regression to predict the continuous probability, ranging from 0 to 1, of a data sample belonging to a specific category, or class. Then, based on that probability, the sample is classified as belonging to the more probable class, ultimately making Logistic Regression a classification algorithm.\n",
    "\n",
    "In our spam filtering example, a Logistic Regression model would predict the probability of an incoming email being spam. If that predicted probability is greater than or equal to 0.5, the email is classified as spam. We would call spam the positive class, with the label 1, since the positive class is the class our model is looking to detect. If the predicted probability is less than 0.5, the email is classified as ham (a real email). We would call ham the negative class, with the label 0. This act of deciding which of two classes a data sample belongs to is called binary classification.\n",
    "\n",
    "Some other examples of what we can classify with Logistic Regression include:\n",
    "\n",
    "Disease survival —Will a patient, 5 years after treatment for a disease, still be alive?\n",
    "Customer conversion —Will a customer arriving on a sign-up page enroll in a service?\n",
    "In this lesson you will learn how to perform Logistic Regression and use it to make classifications on your own data!\n",
    "\n",
    "If you are unfamiliar with Linear Regression, we recommend you go check out our Linear Regression course before proceeding to Logistic Regression. If you are familiar, let’s dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "Codecademy University’s Data Science department is interested in creating a model to predict whether or not a student will pass the final exam of its Introductory Machine Learning course. The department thinks a Logistic Regression model that makes predictions based on the number of hours a student studies will work well. To aid the investigation, the department asked a supplemental question on the exam: how many hours did you study?\n",
    "\n",
    "Run the code in script.py to plot the data samples. 0 indicates that a student failed the exam, and 1 indicates a student passed the exam.\n",
    "\n",
    "How many hours does a student need to study to pass the exam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f39ec62ee2a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mexam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhours_studied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassed_exam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath_courses_taken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Scatter plot of exam passage vs number of hours studied\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exam'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from exam import hours_studied, passed_exam, math_courses_taken\n",
    "\n",
    "# Scatter plot of exam passage vs number of hours studied\n",
    "plt.scatter(hours_studied.ravel(), passed_exam, color='black', zorder=20)\n",
    "plt.ylabel('passed/failed')\n",
    "plt.xlabel('hours studied')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Approach\n",
    "With the data from Codecademy University, we want to predict whether each student will pass their final exam. And the first step to making that prediction is to predict the probability of each student passing. Why not use a Linear Regression model for the prediction, you might ask? Let’s give it a try.\n",
    "\n",
    "Recall that in Linear Regression, we fit a regression line of the following form to the data:\n",
    "\n",
    "$ y = b_{0} + b_{1}x_{1} + b_{2}x_{2} +\\cdots + b_{n}x_{n}$\n",
    " \n",
    "where\n",
    "\n",
    "* y is the value we are trying to predict\n",
    "* b_0 is the intercept of the regression line\n",
    "* b_1, b_2, … b_n are the coefficients of the features x_1, x_2, … x_n of the regression line\n",
    "\n",
    "For our data points y is either 1 (passing), or 0 (failing), and we have one feature, num_hours_studied. Below we fit a Linear Regression model to our data and plotted the results, with the line of best fit in red.\n",
    "\n",
    "<img src=\"images/linear_regression.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Linear Regression Model on Exam Data\n",
    "A problem quickly arises. For low values of num_hours_studied the regression line predicts negative probabilities of passing, and for high values of num_hours_studied the regression line predicts probabilities of passing greater than 1. These probabilities are meaningless! We get these meaningless probabilities since the output of a Linear Regression model ranges from -∞ to +∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "We saw that the output of a Linear Regression model does not provide the probabilities we need to predict whether a student passes the final exam. Step in Logistic Regression!\n",
    "\n",
    "In Logistic Regression we are also looking to find coefficients for our features, but this time we are fitting a logistic curve to the data so that we can predict probabilities. Described below is an overview of how Logistic Regression works. Don’t worry if something does not make complete sense right away, we will dig into each of these steps in further detail in the remaining exercises!\n",
    "\n",
    "To predict the probability of a data sample belonging to a class, we:\n",
    "\n",
    "*  initialize all feature coefficients and intercept to 0\n",
    "\n",
    "\n",
    "*  multiply each of the feature coefficients by their respective feature value to get what is known as the log-odds\n",
    "\n",
    "\n",
    "*  place the log-odds into the sigmoid function to link the output to the range [0,1], giving us a probability\n",
    "\n",
    "   By comparing the predicted probabilities to the actual classes of our data points, we can evaluate how well our model \n",
    "   \n",
    "   makes predictions and use gradient descent to update the coefficients and find the best ones for our model.\n",
    "\n",
    "To then make a final classification, we use a classification threshold to determine whether the data sample belongs to the positive class or the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'codecademylib3_seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4622dedda534>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcodecademylib3_seaborn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mexam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhours_studied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassed_exam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'codecademylib3_seaborn'"
     ]
    }
   ],
   "source": [
    "import codecademylib3_seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from exam import hours_studied, passed_exam\n",
    "from plotter import plot_data\n",
    "\n",
    "# Create logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(hours_studied,passed_exam)\n",
    "\n",
    "# Plug sample data into fitted model\n",
    "sample_x = np.linspace(-16.65, 33.35, 300).reshape(-1,1)\n",
    "probability = model.predict_proba(sample_x)[:,1]\n",
    "\n",
    "# Function to plot exam data and logistic regression curve\n",
    "plot_data(model)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Lowest and highest probabilities\n",
    "lowest = 0.1\n",
    "highest = .95\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Odds\n",
    "\n",
    "In Linear Regression we multiply the coefficients of our features by their respective feature values and add the intercept, resulting in our prediction, which can range from -∞ to +∞. In Logistic Regression, we make the same multiplication of feature coefficients and feature values and add the intercept, but instead of the prediction, we get what is called the log-odds.\n",
    "\n",
    "The log-odds are another way of expressing the probability of a sample belonging to the positive class, or a student passing the exam. In probability, we calculate the odds of an event occurring as follows:\n",
    "\n",
    "$ Odds = \\frac{P(event\\ occurring)}{P(event\\ not\\ occurring)}$ \n",
    "\n",
    "\t \n",
    "The odds tell us how many more times likely an event is to occur than not occur. If a student will pass the exam with probability 0.7, they will fail with probability 1 - 0.7 = 0.3. We can then calculate the odds of passing as:\n",
    "\n",
    "$Odds\\ of\\ passing = \\frac{0.7}{0.3} = 2.\\overline{33}$\n",
    "\n",
    "The log-odds are then understood as the logarithm of the odds!\n",
    "\n",
    "$ Log\\ odds\\ of\\ passing = log(2.\\overline{33}) = 0.847 $\n",
    "\n",
    "\n",
    "For our Logistic Regression model, however, we calculate the log-odds, represented by z below, by summing the product of each feature value by its respective coefficient and adding the intercept. This allows us to map our feature values to a measure of how likely it is that a data sample belongs to the positive class.\n",
    "\n",
    "$z = b_{0}+b_{1}x_{1} + \\cdots + b_{n}x_{n}$\t \n",
    "\n",
    "b_0 is the intercept\n",
    "\n",
    "b_1, b_2, … b_n are the coefficients of the features x_1, x_2, … x_n\n",
    "\n",
    "This kind of multiplication and summing is known as a dot product.\n",
    "\n",
    "We can perform a dot product using numpy‘s np.dot() method! Given feature matrix features, coefficient vector coefficients, and an intercept, we can calculate the log-odds in numpy as follows:\n",
    "\n",
    "log_odds = np.dot(features, coefficients) + intercept\n",
    "np.dot() will take each row, or student, in features and multiply each individual feature value by its respective coefficient in coefficients, summing the result, as shown below.\n",
    "\n",
    "<img src=\"images/matrix-multiplication.gif\" style=\"width:600px;\"/>\n",
    "\n",
    "Matrix Multiplication\n",
    "We then add in the intercept to get the log-odds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "1.\n",
    "Let’s create a function log_odds that takes features, coefficients and intercept as parameters. For now return features.\n",
    "\n",
    "2.\n",
    "Update log_odds to return the dot product of features and coefficients.\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Update the return statement of log-odds by adding the intercept after the dot product.\n",
    "\n",
    "4.\n",
    "With the log_odds function you created, let’s calculate the log-odds of passing for the Introductory Machine Learning students. Use hours_studied as the features, calculated_coefficients as the coefficients and intercept as the intercept. Store the result in calculated_log_odds, and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e6698f5cf022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mexam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhours_studied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcalculated_coefficients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Create your log_odds() function here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlog_odds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mintercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exam'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from exam import hours_studied, calculated_coefficients, intercept\n",
    "\n",
    "# Create your log_odds() function here\n",
    "def log_odds(features,coefficients,intercept):\n",
    "  return  np.dot(features, coefficients) + intercept\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the log-odds for the Codecademy University data here\n",
    "calculated_log_odds= log_odds(hours_studied ,calculated_coefficients,intercept) \n",
    "print(calculated_log_odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Function\n",
    "\n",
    "How did our Logistic Regression model create the S-shaped curve we previously saw? The answer is the Sigmoid Function.\n",
    "\n",
    "Sigmoid Function\n",
    "\n",
    "<img src=\"images/sigmoid.png\" style=\"width:600px;\"/>\n",
    "\n",
    "The Sigmoid Function is a special case of the more general Logistic Function, where Logistic Regression gets its name. Why is the Sigmoid Function so important? By plugging the log-odds into the Sigmoid Function, defined below, we map the log-odds z to the range [0,1].\n",
    "\n",
    "# $h(z)=\\frac{1}{1+e^{-z}}$ \n",
    "\t \n",
    "e^(-z)  is the exponential function, which can be written in numpy as np.exp(-z)\n",
    "This enables our Logistic Regression model to output the probability of a sample belonging to the positive class, or in our case, a student passing the final exam!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "1.\n",
    "Let’s create a Sigmoid Function of our own! Define a function called sigmoid() that takes z as a parameter. For now, have it return z.\n",
    "\n",
    "2.\n",
    "Inside the function and above the return statement, create a variable denominator and set it equal to 1 plus the exponential of -z. Instead of returning z, return 1/denominator.\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "All done! Now test out your function by plugging in the calculated_log_odds we found in the previous exercise and saving the result to probabilities. Then, print probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-57684b512f69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mexam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcalculated_log_odds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Create your sigmoid function here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exam'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from exam import calculated_log_odds\n",
    "\n",
    "# Create your sigmoid function here\n",
    "def sigmoid(z):\n",
    "    denominator = 1 + np.exp(-z)\n",
    "    return 1/denominator\n",
    "\n",
    "# Calculate the sigmoid of the log-odds here\n",
    "probabilities = sigmoid(calculated_log_odds)\n",
    "\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-Loss I\n",
    "\n",
    "Now that we understand how a Logistic Regression model makes its probability predictions, what coefficients and intercept should we use in our model to best predict whether a student will pass the exam? To answer this question we need a way to evaluate how well a given model fits the data we have.\n",
    "\n",
    "The function used to evaluate the performance of a machine learning model is called a loss function, or a cost function. To evaluate how “good a fit” a model is, we calculate the loss for each data sample (how wrong the model’s prediction was) and then average the loss across all samples. The loss function for Logistic Regression, known as Log Loss, is given below:\n",
    "\n",
    "## $-\\frac{1}{m}\\sum_{i=1}^{m} [y^{(i)}log(h(z^{(i)})) + (1-y^{(i)})log(1-h(z^{(i)}))]$\n",
    "\n",
    "* m is the total number of data samples\n",
    "* y_i is the class of data sample i\n",
    "* z_i is the log-odds of sample i\n",
    "* h(z_i) is the sigmoid of the log-odds of sample i, which is the probability of sample i belonging to the positive class\n",
    "\n",
    "The log-loss function might seem scary, but don’t worry, we are going to break it down in the next exercise!\n",
    "\n",
    "The goal of our Logistic Regression model is to find the feature coefficients and intercept, which shape the logistic function, that minimize log-loss for our training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Loss II\n",
    "\n",
    "## $J(\\mathbf{b}) = -\\frac{1}{m}\\sum_{i=1}^{m} [y^{(i)}log(h(z^{(i)})) + (1-y^{(i)})log(1-h(z^{(i)}))]$\n",
    "\n",
    "Let’s go ahead and break down our log-loss function into two separate parts so it begins to make more sense. Consider the case when a data sample has class y = 1, or for our data when a student passed the exam. The right-side of the equation drops out because we end up with 1 - 1 (or 0) multiplied by some value. The loss for that individual student becomes:\n",
    "\n",
    "$loss_{y=1} = -log(h(z^{(i)}))$\n",
    "\n",
    "The loss for a student who passed the exam is just the log of the probability the student passed the exam!\n",
    "\n",
    "And for a student who fails the exam, where a sample has class y = 0, the left-side of the equation drops out and the loss for that student becomes:\n",
    "\n",
    "$loss_{y = 0} = -log(1-h(z^{(i)}))$\n",
    "\n",
    "The loss for a student who failed the exam is the log of one minus the probability the student passed the exam, which is just the log of the probability the student failed the exam!\n",
    "\n",
    "Let’s take a closer look at what is going on with our loss function by graphing the loss of individual samples when the class label is y = 1 and y = 0.\n",
    "\n",
    "<img src=\"images/loss-function-graph.png\" style=\"width:600px;\"/>\n",
    "\n",
    "Log Loss for Positive and Negative Samples\n",
    "Let’s go back to our Codecademy University data and consider four possible cases:\n",
    "\n",
    "\n",
    "|Class|Model|Probability y = 1 Correct?|Loss|\n",
    "|---|---|---|---|\n",
    "|y = 1|\tHigh|Yes|  Low |\n",
    "|y = 1|\tLow\t|No\t|  High|\n",
    "|y = 0|\tHigh|No\t|  High|\n",
    "|y = 0|\tLow\t|Yes|  Low |\n",
    "\n",
    "From the graphs and the table you can see that confident correct predictions result in small losses, while confident incorrect predictions result in large losses that approach infinity. This makes sense! We want to punish our model with an increasing loss as it makes progressively incorrect predictions, and we want to reward the model with a small loss as it makes correct predictions.\n",
    "\n",
    "Just like in Linear Regression, we can then use gradient descent to find the coefficients that minimize log-loss across all of our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "1.\n",
    "Let’s calculate the log-loss for our Codecademy University data. To calculate loss we need the actual classes, pass (1), or fail (0), for the students. Print passed_exam to inspect the actual classes.\n",
    "\n",
    "2.\n",
    "In the code editor, we’ve provided you with a function log_loss() that calculates the log-loss for a set of predicted probabilities and their actual classes. Use probabilities, which you calculated previously, and passed_exam as inputs to log_loss() and store the result in loss_1. Print loss_1.\n",
    "\n",
    "\n",
    "\n",
    "3.\n",
    "Now that we have calculated the loss for our best coefficients, let’s compare this loss to the loss we begin with when we initialize our coefficients and intercept to 0. probabilities_2 contains the calculated probabilities of the students passing the exam with the coefficient for hours_studied set to 0. Use probabilities_2 and passed_exam as inputs to log_loss() and store the result in loss_2. Print loss_2.\n",
    "\n",
    "Which set of coefficients produced the lower log-loss?\n",
    "\n",
    "\n",
    "\n",
    "The first set of coefficients produced a lower log-loss than the second set! This is because the first set of coefficients was determined by the Logistic Regression model as the optimized coefficient values. The second set of coefficients were initialized at 0, and served as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6a224ab67225>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mexam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpassed_exam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilities_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Function to calculate log-loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactual_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exam'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from exam import passed_exam, probabilities, probabilities_2\n",
    "\n",
    "# Function to calculate log-loss\n",
    "def log_loss(probabilities,actual_class):\n",
    "  return np.sum(-(1/actual_class.shape[0])*(actual_class*np.log(probabilities) + (1-actual_class)*np.log(1-probabilities)))\n",
    "\n",
    "# Print passed_exam here\n",
    "# print(passed_exam)\n",
    "\n",
    "#print(probabilities)\n",
    "# print(probabilities_2)\n",
    "#print(passed_exam)\n",
    "\n",
    "# Calculate and print loss_1 here\n",
    "loss_1 = log_loss(probabilities,passed_exam)\n",
    "print(loss_1)\n",
    "\n",
    "# Calculate and print loss_2 here\n",
    "loss_2 = log_loss(probabilities_2,passed_exam)\n",
    "print(loss_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Thresholding\n",
    "\n",
    "Many machine learning algorithms, including Logistic Regression, spit out a classification probability as their result. Once we have this probability, we need to make a decision on what class the sample belongs to. This is where the classification threshold comes in!\n",
    "\n",
    "The default threshold for many algorithms is 0.5. If the predicted probability of an observation belonging to the positive class is greater than or equal to the threshold, 0.5, the classification of the sample is the positive class. If the predicted probability of an observation belonging to the positive class is less than the threshold, 0.5, the classification of the sample is the negative class.\n",
    "\n",
    "<img src=\"images/Threshold-01.svg\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "We can choose to change the threshold of classification based on the use-case of our model. For example, if we are creating a Logistic Regression model that classifies whether or not an individual has cancer, we want to be more sensitive to the positive cases, signifying the presence of cancer, than the negative cases.\n",
    "\n",
    "In order to ensure that most patients with cancer are identified, we can move the classification threshold down to 0.3 or 0.4, increasing the sensitivity of our model to predicting a positive cancer classification. While this might result in more overall misclassifications, we are now missing fewer of the cases we are trying to detect: actual cancer patients.\n",
    "\n",
    "<img src=\"images/Threshold-02.svg\" style=\"width:600px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question \n",
    "1.\n",
    "Let’s use all the knowledge we’ve gathered to create a function that performs thresholding and makes class predictions! Define a function predict_class() that takes a features matrix, a coefficients vector, an intercept, and a threshold as parameters. Return threshold.\n",
    "\n",
    "2.\n",
    "In predict_class(), calculate the log-odds using the log_odds() function we defined earlier. Store the result in calculated_log_odds, and return calculated_log_odds.\n",
    "\n",
    "3.\n",
    "Still in predict_class(), find the probabilities that the samples belong to the positive class. Create a variable probabilities, and give it the value returned by calling sigmoid() on calculated_log_odds. Return probabilities.\n",
    "\n",
    "4.\n",
    "Return 1 for all values within probabilities equal to or above threshold, and 0 for all values below threshold.\n",
    "\n",
    "\n",
    "\n",
    "5.\n",
    "Let’s make final classifications on our Codecademy University data to see which students passed the exam. Use the predict_class() function with hours_studied, calculated_coefficients, intercept, and a threshold of 0.5 as parameters. Store the results in final_results, and print final_results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6d4b2cb063bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mexam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhours_studied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcalculated_coefficients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlog_odds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mintercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exam'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from exam import hours_studied, calculated_coefficients, intercept\n",
    "\n",
    "def log_odds(features, coefficients,intercept):\n",
    "  return np.dot(features,coefficients) + intercept\n",
    "\n",
    "def sigmoid(z):\n",
    "    denominator = 1 + np.exp(-z)\n",
    "    return 1/denominator\n",
    "\n",
    "# Create predict_class() function here\n",
    "\n",
    "def predict_class(features,coefficients,intercept,threshold):\n",
    "  calculated_log_odds = log_odds(features,coefficients,intercept)\n",
    "  probabilities = sigmoid(calculated_log_odds)\n",
    "  return np.where(probabilities >= threshold, 1, 0)\n",
    "\n",
    "# Make final classifications on Codecademy University data here\n",
    "\n",
    "final_results = predict_class(hours_studied, calculated_coefficients, intercept, 0.5)\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn\n",
    "\n",
    "Now that you know the inner workings of how Logistic Regression works, let’s learn how to easily and quickly create Logistic Regression models with sklearn! sklearn is a Python library that helps build, train, and evaluate Machine Learning models.\n",
    "\n",
    "To take advantage of sklearn‘s abilities, we can begin by creating a LogisticRegression object.\n",
    "\n",
    "* model = LogisticRegression()\n",
    "\n",
    "After creating the object, we need to fit our model on the data. When we fit the model with sklearn it will perform gradient descent, repeatedly updating the coefficients of our model in order to minimize the log-loss. We train — or fit — the model using the .fit() method, which takes two parameters. The first is a matrix of features, and the second is a matrix of class labels.\n",
    "\n",
    "* model.fit(features, labels)\n",
    "\n",
    "Now that the model is trained, we can access a few useful attributes of the LogisticRegression object.\n",
    "\n",
    "* model.coef_ is a vector of the coefficients of each feature\n",
    "* model.intercept_ is the intercept b_0\n",
    "\n",
    "With our trained model we are able to predict whether new data points belong to the positive class using the .predict() method! .predict() takes a matrix of features as a parameter and returns a vector of labels 1 or 0 for each sample. In making its predictions, sklearn uses a classification threshold of 0.5.\n",
    "\n",
    "* model.predict(features)\n",
    "\n",
    "If we are more interested in the predicted probability of the data samples belonging to the positive class than the actual class, we can use the .predict_proba() method. predict_proba() also takes a matrix of features as a parameter and returns a vector of probabilities, ranging from 0 to 1, for each sample.\n",
    "\n",
    "* model.predict_proba(features)\n",
    "\n",
    "Before proceeding, one important note is that sklearn‘s Logistic Regression implementation requires feature data to be normalized. Normalization scales all feature data to vary over the same range. sklearn‘s Logistic Regression requires normalized feature data due to a technique called Regularization that it uses under the hood. Regularization is out of the scope of this lesson, but in order to ensure the best results from our model, we will be using a normalized version of the data from our Codecademy University example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "1.\n",
    "Let’s build, train and evaluate a Logistic Regression model in sklearn for our Codecademy University data! We’ve imported sklearn and the LogisiticRegression classifier for you. Create a Logistic Regression model named model.\n",
    "\n",
    "2.\n",
    "Train the model using hours_studied_scaled as the training features and passed_exam as the training labels.\n",
    "\n",
    "3.\n",
    "Save the coefficients of the model to the variable calculated_coefficients, and the intercept of the model to intercept. Print calculated_coefficients and intercept.\n",
    "\n",
    "4.\n",
    "The next semester a group of students in the Introductory Machine Learning course want to predict their final exam scores based on how much they intended to study for the exam. The number of hours each student thinks they will study, normalized, is given in guessed_hours_scaled. Use model to predict the probability that each student will pass the final exam, and save the probabilities to passed_predictions.\n",
    "\n",
    "5.\n",
    "That same semester, the Data Science department decides to update the final exam passage model to consider two features instead of just one. During the final exam, students were asked to estimate how much time they spent studying, as well as how many previous math courses they have taken. The student responses, along with their exam results, were split into training and test sets. The training features, normalized, are given to you in exam_features_scaled_train, and the students’ results on the final are given in passed_exam_2_train.\n",
    "\n",
    "Create a new Logistic Regression model named model_2 and train it on exam_features_scaled_train and passed_exam_2_train.\n",
    "\n",
    "6.\n",
    "Use the model you just trained to predict whether each student in the test set, exam_features_scaled_test, will pass the exam and save the predictions to passed_predictions_2. Print passed_predictions_2.\n",
    "\n",
    "Compare the predictions to the actual student performance on the exam in the test set. How well did your model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d5b11112eb0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mexam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhours_studied_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassed_exam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexam_features_scaled_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexam_features_scaled_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassed_exam_2_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassed_exam_2_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mguessed_hours_scaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create and fit logistic regression model here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exam'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from exam import hours_studied_scaled, passed_exam, exam_features_scaled_train, exam_features_scaled_test, passed_exam_2_train, passed_exam_2_test, guessed_hours_scaled\n",
    "\n",
    "# Create and fit logistic regression model here\n",
    "model = LogisticRegression()\n",
    "model.fit(hours_studied_scaled,passed_exam)\n",
    "\n",
    "# Save the model coefficients and intercept here\n",
    "calculated_coefficients = model.coef_\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(calculated_coefficients)\n",
    "print(intercept)\n",
    "\n",
    "# Predict the probabilities of passing for next semester's students here\n",
    "passed_predictions = model.predict_proba(guessed_hours_scaled)\n",
    "\n",
    "# Create a new model on the training data with two features here\n",
    "model_2 = LogisticRegression()\n",
    "model_2.fit(exam_features_scaled_train,passed_exam_2_train)\n",
    "\n",
    "# Predict whether the students will pass here\n",
    "passed_predictions_2 = model_2.predict(exam_features_scaled_test)\n",
    "\n",
    "print(passed_predictions_2)\n",
    "print(passed_exam_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "One of the defining features of Logistic Regression is the interpretability we have from the feature coefficients. How to handle interpreting the coefficients depends on the kind of data you are working with (normalized or not) and the specific implementation of Logistic Regression you are using. We’ll discuss how to interpret the feature coefficients from a model created in sklearn with normalized feature data.\n",
    "\n",
    "Since our data is normalized, all features vary over the same range. Given this understanding, we can compare the feature coefficients’ magnitudes and signs to determine which features have the greatest impact on class prediction, and if that impact is positive or negative.\n",
    "\n",
    "* Features with larger, positive coefficients will increase the probability of a data sample belonging to the positive class\n",
    "* Features with larger, negative coefficients will decrease the probability of a data sample belonging to the positive class\n",
    "* Features with small, positive or negative coefficients have minimal impact on the probability of a data sample belonging to the positive class\n",
    "\n",
    "Given cancer data, a logistic regression model can let us know what features are most important for predicting survival after, for example, five years from diagnosis. Knowing these features can lead to a better understanding of outcomes, and even lives saved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "1.\n",
    "Let’s revisit the sklearn Logistic Regression model we fit to our exam data in the last exercise. Remember, the two features in the new model are the number of hours studied and the number of previous math courses taken.\n",
    "\n",
    "Using the model, given to you as model_2 in the code editor, save the feature coefficients to the variable coefficients.\n",
    "\n",
    "2.\n",
    "In order to visualize the coefficients, let’s pull them out of the numpy array in which they are currently stored. With numpys tolist() method we can convert the array into a list and grab the values we want to visualize.\n",
    "\n",
    "Below your original assignment of coefficients, update coefficients to equal coefficients.tolist()[0].\n",
    "\n",
    "3.\n",
    "Create a bar graph comparing the feature coefficients with matplotlib‘s plt.bar() method. Which feature appears to be more important in determining whether or not a student will pass the Introductory Machine Learning final exam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-f7103919247c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mexam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexam_features_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassed_exam_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Train a sklearn logistic regression model on the normalized exam data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exam'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from exam import exam_features_scaled, passed_exam_2\n",
    "\n",
    "# Train a sklearn logistic regression model on the normalized exam data\n",
    "model_2 = LogisticRegression()\n",
    "model_2.fit(exam_features_scaled,passed_exam_2)\n",
    "\n",
    "# Assign and update coefficients\n",
    "coefficients = model_2.coef_\n",
    "coefficients =  coefficients.tolist()[0]\n",
    "print(coefficients)\n",
    "\n",
    "# Plot bar graph\n",
    "\n",
    "plt.bar([1,2],coefficients)\n",
    "\n",
    "plt.xticks([1,2],['hours studied','math courses taken'])\n",
    "plt.xlabel('feature')\n",
    "plt.ylabel('coefficient')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "Congratulations! You just learned how a Logistic Regression model works and how to fit one to a dataset. Class is over, and the final exam for Codecademy University’s Introductory Machine Learning is around the corner. Do you predict that you will pass? Let’s do some review to make sure.\n",
    "\n",
    "* Logistic Regression is used to perform binary classification, predicting whether a data sample belongs to a positive (present) class, labeled 1 and the negative (absent) class, labeled 0.\n",
    "\n",
    "* The Sigmoid Function bounds the product of feature values and their coefficients, known as the log-odds, to the range [0,1], providing the probability of a sample belonging to the positive class.\n",
    "\n",
    "* A loss function measures how well a machine learning model makes predictions. The loss function of Logistic Regression is log-loss.\n",
    "\n",
    "* A Classification Threshold is used to determine the probabilistic cutoff for where a data sample is classified as belonging to a positive or negative class. The standard cutoff for Logistic Regression is 0.5, but the threshold can be higher or lower depending on the nature of the data and the situation.\n",
    "\n",
    "* Scikit-learn has a Logistic Regression implementation that allows you to fit a model to your data, find the feature coefficients, and make predictions on new data samples.\n",
    "\n",
    "* The coefficients determined by a Logistic Regression model can be used to interpret the relative importance of each feature in predicting the class of a data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
